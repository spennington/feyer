{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN\n",
    "While our previous model is training let's look into RNNs which seem like they would be a better fit for our problem.\n",
    "\n",
    "Working off of https://pytorch.org/tutorials/intermediate/char_rnn_classification_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "from trainer.dataset import CrosswordClueAnswersDataset\n",
    "\n",
    "PADDING_TOKEN_INDEX = 0\n",
    "PAD_TO_SIZE = 10\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.rnn = nn.RNN(input_size=input_size, hidden_size=hidden_size, num_layers=1, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    def forward(self, input):\n",
    "        h0 = Variable(torch.zeros(1, input.size(0), self.hidden_size))\n",
    "        output, hn = self.rnn(input, h0)\n",
    "        output = self.fc(output[:, -1, :]) \n",
    "        return self.softmax(output)\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, self.hidden_size)\n",
    "        \n",
    "def run(args):\n",
    "    device = 'cpu'\n",
    "    print(f'Running on:', device)\n",
    "\n",
    "    # load data, split datasets, build vocabs\n",
    "    #TODO: Parameterize this\n",
    "    dataset = CrosswordClueAnswersDataset(\"cleaned_data/no_dupes_10_or_less_tokens.csv\")\n",
    "    train_size = int(0.01 * len(dataset))\n",
    "    dev_size = int(0.001 * len(dataset))\n",
    "    test_size = len(dataset) - train_size - dev_size\n",
    "    g = torch.Generator().manual_seed(42) # this manual_seed is important to ensure that we consistently split the dataset\n",
    "    train_dataset, test_dataset, dev_dataset = torch.utils.data.random_split(dataset, [train_size, test_size, dev_size], generator=g)\n",
    "\n",
    "    # build vocab onlt off of training data (for now...)\n",
    "    tokenizer = get_tokenizer('basic_english')\n",
    "\n",
    "    clues_iter = map(lambda data: tokenizer(data[1]), train_dataset)\n",
    "    answers_iter = map(lambda data: tokenizer(data[0]), train_dataset)\n",
    "    \n",
    "    clues_vocab = build_vocab_from_iterator(clues_iter, specials=['<pad>', '<unk>'])\n",
    "    clues_vocab.set_default_index(1)\n",
    "\n",
    "    answers_vocab = build_vocab_from_iterator(answers_iter, specials=['<unk>'])\n",
    "    answers_vocab.set_default_index(0)\n",
    "\n",
    "    print(f'{len(dataset)=}\\n{len(train_dataset)=}\\n{len(test_dataset)=}\\n{len(dev_dataset)=}')\n",
    "    print(f'{len(answers_vocab)=}\\n{len(clues_vocab)=}')\n",
    "\n",
    "    def collate_batch(batch):\n",
    "        answer_list, clue_list = [], []\n",
    "\n",
    "        for (answer, clue) in batch:\n",
    "            clue_indicies = clues_vocab(tokenizer(clue))\n",
    "            clue_indicies += [PADDING_TOKEN_INDEX] * (PAD_TO_SIZE - len(clue_indicies))\n",
    "            clue_list.append(clue_indicies)\n",
    "\n",
    "            answer_list.append(answers_vocab([answer])[0])\n",
    "\n",
    "        answer_list = torch.tensor(answer_list).to(device)\n",
    "        clue_list = torch.tensor(clue_list).to(device)\n",
    "\n",
    "        return answer_list, clue_list\n",
    "\n",
    "    # shuffle the training dataloader so we go through different batches each time\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "    dev_dataloader = DataLoader(dev_dataset, batch_size=args.batch_size, shuffle=False, collate_fn=collate_batch) \n",
    "\n",
    "    print(f'{len(train_dataloader)=}\\n{len(dev_dataloader)=}')\n",
    "\n",
    "    n_hidden = 128\n",
    "    input_size = len(clues_vocab)\n",
    "    n_categories = len(answers_vocab)\n",
    "    rnn = RNN(input_size, n_hidden, n_categories)\n",
    "\n",
    "    data_item = next(iter(train_dataloader))\n",
    "    print(f'{data_item=}')\n",
    "    clue = data_item[1]\n",
    "\n",
    "    # Turn a clue into a <line_length x 1 x n_letters>,\n",
    "    # or an array of one-hot letter vectors\n",
    "    def clue_to_tensor(clue):\n",
    "        tensor = torch.zeros(len(clue), 1, len(clues_vocab))\n",
    "        for li, token_index in enumerate(clue):\n",
    "            tensor[li][0][token_index] = 1\n",
    "        return tensor\n",
    "\n",
    "    learning_rate = 0.005 # If you set this too high, it might explode. If too low, it might not learn\n",
    "    criterion = nn.NLLLoss()\n",
    "\n",
    "    def train(answer_tensor, clue_tensor, optimizer):\n",
    "        rnn.zero_grad()\n",
    "        output = rnn(clue_tensor)\n",
    "        loss = criterion(output, answer_tensor)\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Add parameters' gradients to their values, multiplied by learning rate\n",
    "        # for p in rnn.parameters():\n",
    "        #     p.data.add_(p.grad.data, alpha=-learning_rate)\n",
    "\n",
    "        return output, loss.item()\n",
    "\n",
    "    import time\n",
    "    import math\n",
    "\n",
    "    epochs = 10\n",
    "    print_every = 1000\n",
    "    plot_every = 100\n",
    "\n",
    "    # Keep track of losses for plotting\n",
    "    current_loss = 0\n",
    "    all_losses = []\n",
    "\n",
    "    optimizer = torch.optim.AdamW(rnn.parameters())\n",
    "\n",
    "    def timeSince(since):\n",
    "        now = time.time()\n",
    "        s = now - since\n",
    "        m = math.floor(s / 60)\n",
    "        s -= m * 60\n",
    "        return '%dm %ds' % (m, s)\n",
    "    \n",
    "    def categoryFromOutput(output):\n",
    "        top_n, top_i = output.topk(1)\n",
    "        category_i = top_i[0].item()\n",
    "        return category_i\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    for i in range(epochs):\n",
    "        for idx, (answer, clue) in enumerate(train_dataloader):\n",
    "            clue_tensor = clue_to_tensor(clue[0])\n",
    "            output, loss = train(answer, clue_tensor, optimizer)\n",
    "            current_loss += loss\n",
    "\n",
    "            # Print iter number, loss, name and guess\n",
    "            if idx % print_every == 0:\n",
    "                guess_i = categoryFromOutput(output)\n",
    "                correct = '✓' if guess_i == answer else '✗ (%s)' % answer\n",
    "                print('%d %d%% (%s) %.4f / %s' % (idx, idx / len(train_dataloader) * 100, timeSince(start), loss, correct))\n",
    "\n",
    "            # Add current loss avg to list of losses\n",
    "            if idx % plot_every == 0:\n",
    "                all_losses.append(current_loss / plot_every)\n",
    "                current_loss = 0\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    # print(one_hot[0][41])\n",
    "    \n",
    "    # answer = data_item[0]\n",
    "    # print(f'{clue=}')\n",
    "    # print(f'{answer=}')\n",
    "\n",
    "    \n",
    "    # output, next_hidden = rnn(one_hot, hidden)\n",
    "    # print(f'{output.shape=}')\n",
    "    # print(f'{next_hidden.shape=}')\n",
    "\n",
    "\n",
    "    # model = SimpleCrosswordModel(\n",
    "    #     vocab_size=len(clues_vocab),\n",
    "    #     embed_dim=args.embedding_dimensions,\n",
    "    #     input_size=PAD_TO_SIZE,\n",
    "    #     hidden_size=args.hidden_layer_size,\n",
    "    #     output_size=len(answers_vocab),\n",
    "    #     device=device,\n",
    "    #     hidden_depth=args.hidden_depth)\n",
    "\n",
    "    # trainable_model_parameters = filter(lambda p: p.requires_grad, model.parameters())\n",
    "    # params_count = sum(p.numel() for p in model.parameters())\n",
    "    # trainable_params_count = sum(p.numel() for p in trainable_model_parameters)\n",
    "    # print(f'{params_count=}\\n{trainable_params_count=}')\n",
    "\n",
    "    # criterion = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "    # optimizer = torch.optim.AdamW(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)\n",
    "\n",
    "    # train_dir = os.path.join('training_results', args.output_folder)\n",
    "    # if not os.path.exists(train_dir):\n",
    "    #     os.makedirs(train_dir)\n",
    "    # print(f'Outputting to: {train_dir}')\n",
    "\n",
    "    # log_interval = int(len(train_dataloader) / 5)\n",
    "    # reporter = TrainingReporter(train_dir, log_interval)\n",
    "    # training = Trainer(model=model, criterion=criterion, optimizer=optimizer, reporter=reporter, output_dir=train_dir)\n",
    "    # training.start(num_epochs=args.num_epochs, train_dataloader=train_dataloader, test_dataloader=dev_dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: cpu\n",
      "len(dataset)=538925\n",
      "len(train_dataset)=5389\n",
      "len(test_dataset)=532998\n",
      "len(dev_dataset)=538\n",
      "len(answers_vocab)=4607\n",
      "len(clues_vocab)=6848\n",
      "len(train_dataloader)=5389\n",
      "len(dev_dataloader)=538\n",
      "data_item=(tensor([379]), tensor([[  48,   13, 1644,  975,    0,    0,    0,    0,    0,    0]]))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected input batch_size (10) to match target batch_size (1).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39margparse\u001b[39;00m \u001b[39mimport\u001b[39;00m Namespace\n\u001b[1;32m      2\u001b[0m args \u001b[39m=\u001b[39m Namespace(batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m run(args)\n",
      "Cell \u001b[0;32mIn[9], line 144\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[39mfor\u001b[39;00m idx, (answer, clue) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(train_dataloader):\n\u001b[1;32m    143\u001b[0m     clue_tensor \u001b[39m=\u001b[39m clue_to_tensor(clue[\u001b[39m0\u001b[39m])\n\u001b[0;32m--> 144\u001b[0m     output, loss \u001b[39m=\u001b[39m train(answer, clue_tensor, optimizer)\n\u001b[1;32m    145\u001b[0m     current_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\n\u001b[1;32m    147\u001b[0m     \u001b[39m# Print iter number, loss, name and guess\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[9], line 103\u001b[0m, in \u001b[0;36mrun.<locals>.train\u001b[0;34m(answer_tensor, clue_tensor, optimizer)\u001b[0m\n\u001b[1;32m    101\u001b[0m rnn\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m    102\u001b[0m output \u001b[39m=\u001b[39m rnn(clue_tensor)\n\u001b[0;32m--> 103\u001b[0m loss \u001b[39m=\u001b[39m criterion(output, answer_tensor)\n\u001b[1;32m    104\u001b[0m loss\u001b[39m.\u001b[39mbackward()\n\u001b[1;32m    106\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ml_dev/lib/python3.9/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ml_dev/lib/python3.9/site-packages/torch/nn/modules/loss.py:216\u001b[0m, in \u001b[0;36mNLLLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 216\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mnll_loss(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ml_dev/lib/python3.9/site-packages/torch/nn/functional.py:2704\u001b[0m, in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[1;32m   2702\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2703\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 2704\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mnll_loss_nd(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index)\n",
      "\u001b[0;31mValueError\u001b[0m: Expected input batch_size (10) to match target batch_size (1)."
     ]
    }
   ],
   "source": [
    "from argparse import Namespace\n",
    "args = Namespace(batch_size=1)\n",
    "run(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "42b85a257f5624c76500f25a969244d887270373d8988c1b039e87d3dd5d8436"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
