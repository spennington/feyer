{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Character Level Model\n",
    "We didn't see much success with our previous models. We topped out at about 40% accuracy even having duplicates in the dataset.\n",
    "\n",
    "My theory is that the model struggles because the input and output spaces are so large: ~77k input tokens ~60k output tokens. This means the network must be quite large which makes it difficult to train.\n",
    "\n",
    "Next, I'd like to recondiser this problem as a character level problem. Given the answer,clue pair `neural,kind of network` we can consider the problem on a character by character basis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "||||| k --> i --> n --> d -->   --> o --> f -->   --> n --> e --> t --> w --> o --> r --> k --> ||||| n --> e --> u --> r --> a --> l --> ||||| \n"
     ]
    }
   ],
   "source": [
    "answer = 'neural'\n",
    "clue = 'kind of network'\n",
    "\n",
    "print('||||| ', end='')\n",
    "\n",
    "for c in clue:\n",
    "    print(f'{c} --> ', end='')\n",
    "\n",
    "print('||||| ', end='')\n",
    "\n",
    "for c in answer:\n",
    "    print(f'{c} --> ', end='')\n",
    "\n",
    "print('||||| ')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's replace those `|||||` separators with things that are meaningful. We'll use `<BOC>`, `<EOC>`, and `<EOA>` tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOC> --> n --> e --> u --> r --> a --> l --> <EOC> --> n --> e --> u --> r --> a --> l --> <EOA>"
     ]
    }
   ],
   "source": [
    "tokens = ['<BOC>']\n",
    "tokens.extend(list(answer))\n",
    "tokens.append('<EOC>')\n",
    "tokens.extend(list(answer))\n",
    "tokens.append('<EOA>')\n",
    "\n",
    "for i, token in enumerate(tokens):\n",
    "    print(f'{token}', end='')\n",
    "    if i < len(tokens) - 1:\n",
    "        print(' --> ', end='')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So that looks good, but let's think about the `<EOC>` token - what if we replaced that with a special token indicating the _length of the expected answer_. This serves as a useful indicator to the network as to what should be generated next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<BOC> --> k --> i --> n --> d -->   --> o --> f -->   --> n --> e --> t --> w --> o --> r --> k --> <6> --> n --> e --> u --> r --> a --> l --> <EOA>"
     ]
    }
   ],
   "source": [
    "tokens = ['<BOC>']\n",
    "tokens.extend(list(clue))\n",
    "tokens.append('<' + str(len(answer)) + '>')\n",
    "tokens.extend(list(answer))\n",
    "tokens.append('<EOA>')\n",
    "\n",
    "for i, token in enumerate(tokens):\n",
    "    print(f'{token}', end='')\n",
    "    if i < len(tokens) - 1:\n",
    "        print(' --> ', end='')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with our Dataset\n",
    "Let's load up a few data rows and see how this would work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer</th>\n",
       "      <th>clue</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pat</td>\n",
       "      <td>action done while saying \"good dog\"</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rascals</td>\n",
       "      <td>mischief-makers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pen</td>\n",
       "      <td>it might click for a writer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>sep</td>\n",
       "      <td>fall mo.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>eco</td>\n",
       "      <td>kind to mother nature</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    answer                                 clue\n",
       "0      pat  action done while saying \"good dog\"\n",
       "1  rascals                      mischief-makers\n",
       "2      pen          it might click for a writer\n",
       "3      sep                             fall mo.\n",
       "4      eco                kind to mother nature"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"cleaned_data/dupes_10_or_less_tokens.csv\", keep_default_na=False)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<BOC>', 'a', 'c', 't', 'i', 'o', 'n', ' ', 'd', 'o', 'n', 'e', ' ', 'w', 'h', 'i', 'l', 'e', ' ', 's', 'a', 'y', 'i', 'n', 'g', ' ', '\"', 'g', 'o', 'o', 'd', ' ', 'd', 'o', 'g', '\"', '<3>', 'p', 'a', 't', '<EOA>']\n",
      "['<BOC>', 'm', 'i', 's', 'c', 'h', 'i', 'e', 'f', '-', 'm', 'a', 'k', 'e', 'r', 's', '<7>', 'r', 'a', 's', 'c', 'a', 'l', 's', '<EOA>']\n",
      "['<BOC>', 'i', 't', ' ', 'm', 'i', 'g', 'h', 't', ' ', 'c', 'l', 'i', 'c', 'k', ' ', 'f', 'o', 'r', ' ', 'a', ' ', 'w', 'r', 'i', 't', 'e', 'r', '<3>', 'p', 'e', 'n', '<EOA>']\n",
      "['<BOC>', 'f', 'a', 'l', 'l', ' ', 'm', 'o', '.', '<3>', 's', 'e', 'p', '<EOA>']\n",
      "['<BOC>', 'k', 'i', 'n', 'd', ' ', 't', 'o', ' ', 'm', 'o', 't', 'h', 'e', 'r', ' ', 'n', 'a', 't', 'u', 'r', 'e', '<3>', 'e', 'c', 'o', '<EOA>']\n"
     ]
    }
   ],
   "source": [
    "def build_tokens(row):\n",
    "    clue = row['clue']\n",
    "    answer = row['answer']\n",
    "    tokens = ['<BOC>']\n",
    "    tokens.extend(list(clue))\n",
    "    tokens.append('<' + str(len(answer)) + '>')\n",
    "    tokens.extend(list(answer))\n",
    "    tokens.append('<EOA>')\n",
    "    return tokens\n",
    "\n",
    "token_batch = []\n",
    "for i in range(5):\n",
    "    tokens = build_tokens(df.iloc[i])\n",
    "    token_batch.append(tokens)\n",
    "    print(tokens)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks good, let's try this out with the first few rows in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows = df.iloc[:100]\n",
    "all_chars = ''.join(rows['answer']) + ''.join(rows['clue'])\n",
    "\n",
    "vocab = sorted(list(set(all_chars)))\n",
    "vocab += ['<BOC>', '<EOA>']\n",
    "\n",
    "max_answer_length = rows['answer'].str.len().max()\n",
    "for i in range(1, max_answer_length + 1):\n",
    "    vocab.append(f'<{i}>')\n",
    "\n",
    "vocab_stoi = {s:i+1 for i,s in enumerate(vocab)}\n",
    "vocab_itos = {i:s for s,i in vocab_stoi.items()}\n",
    "vocab_size = len(vocab)\n",
    "vocab_size"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our vocabulary built let's build a model for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TextGenerator(nn.Module):\n",
    "    def __init__(self, vocab_size, hidden_size, n_layers=1):\n",
    "        super(TextGenerator, self).__init__()\n",
    "\n",
    "        # identiy matrix for generating one-hot vectors\n",
    "        self.ident = torch.eye(vocab_size)\n",
    "\n",
    "        # recurrent neural network\n",
    "        self.rnn = nn.GRU(vocab_size, hidden_size, n_layers, batch_first=True)\n",
    "\n",
    "        # a fully-connect layer that outputs a distribution over\n",
    "        # the next token, given the RNN output\n",
    "        self.decoder = nn.Linear(hidden_size, vocab_size)\n",
    "    \n",
    "    def forward(self, inp, hidden=None):\n",
    "        inp = self.ident[inp]                  # generate one-hot vectors of input\n",
    "        output, hidden = self.rnn(inp, hidden) # get the next output and hidden state\n",
    "        output = self.decoder(output)          # predict distribution over next tokens\n",
    "        return output, hidden\n",
    "\n",
    "model = TextGenerator(vocab_size, 64)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what our network will produce given the start token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0178,  0.0924, -0.0770,  0.0569, -0.0954,  0.1026, -0.0615,\n",
       "          -0.1311,  0.0406,  0.0392,  0.0010, -0.0470, -0.0253,  0.0990,\n",
       "           0.1132,  0.0917, -0.0628, -0.1088, -0.0408,  0.1090, -0.0776,\n",
       "          -0.1293,  0.0132, -0.0651, -0.0441, -0.1235,  0.0489, -0.0926,\n",
       "          -0.0686, -0.1014, -0.0057, -0.1262,  0.1100,  0.0821, -0.0654,\n",
       "          -0.1416, -0.1254, -0.0809,  0.0261, -0.0597, -0.0665, -0.1357,\n",
       "           0.1129, -0.0992,  0.0552, -0.0168, -0.0897,  0.0574, -0.0602,\n",
       "           0.0288, -0.0957, -0.0481,  0.1097]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "bos_input = torch.Tensor([vocab_stoi['<BOC>']]).long().unsqueeze(0)\n",
    "output, hidden = model(bos_input, hidden=None)\n",
    "output # distribution over the first token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.9763, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "first_clue = rows['clue'][0]\n",
    "first_answer = rows['answer'][0]\n",
    "target = torch.Tensor([vocab_stoi[first_clue[0]]]).long().unsqueeze(0)\n",
    "criterion(output.reshape(-1, vocab_size), # reshape to 2D tensor\n",
    "          target.reshape(-1))             # reshape to 1D tensor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of passing this token back to the NN to genereate the next one, we actually pass in the ground truth data and see the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0246,  0.1438, -0.0671,  0.0665, -0.0980,  0.1539, -0.0488,\n",
       "          -0.1359,  0.0687, -0.0104, -0.0483, -0.0631, -0.0049,  0.0842,\n",
       "           0.1212,  0.1134, -0.0409, -0.0921, -0.0227,  0.1279, -0.0951,\n",
       "          -0.1102,  0.0191, -0.0683, -0.0436, -0.1478,  0.0586, -0.1148,\n",
       "          -0.0461, -0.0801, -0.0176, -0.1271,  0.0776,  0.0634, -0.1016,\n",
       "          -0.1497, -0.1664, -0.0726,  0.0216, -0.0529, -0.0896, -0.1100,\n",
       "           0.1092, -0.0889,  0.0584, -0.0679, -0.0795,  0.0676, -0.0585,\n",
       "           0.0418, -0.0949, -0.0583,  0.1216]]], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use teacher-forcing: we pass in the ground truth `target`,\n",
    "# rather than using the NN predicted distribution\n",
    "output, hidden = model(target, hidden)\n",
    "output # distribution over the second token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.8305, grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = torch.Tensor([vocab_stoi[first_clue[1]]]).long().unsqueeze(0)\n",
    "criterion(output.reshape(-1, vocab_size), # reshape to 2D tensor\n",
    "          target.reshape(-1))             # reshape to 1D tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 tensor([[[-0.0317,  0.1444, -0.0016,  0.0600, -0.1215,  0.1242, -0.0587,\n",
      "          -0.1374,  0.1119,  0.0735,  0.0022, -0.0196, -0.0028,  0.0944,\n",
      "           0.1236,  0.1139, -0.1002, -0.0901, -0.0004,  0.1442, -0.1161,\n",
      "          -0.0799,  0.0437, -0.1013, -0.0683, -0.1529,  0.0656, -0.1302,\n",
      "          -0.0156, -0.0974,  0.0040, -0.1278,  0.0999,  0.0033, -0.0997,\n",
      "          -0.1592, -0.1424, -0.0563,  0.0772, -0.0236, -0.0481, -0.1121,\n",
      "           0.1024, -0.0921,  0.0446,  0.0118, -0.1160,  0.0841, -0.1158,\n",
      "           0.0573, -0.1103, -0.0902,  0.0991]]], grad_fn=<ViewBackward0>) tensor(4.0847, grad_fn=<NllLossBackward0>)\n",
      "3 tensor([[[-0.0092,  0.1451,  0.0026,  0.0823, -0.1214,  0.1331, -0.0897,\n",
      "          -0.1330,  0.0821,  0.0672, -0.0024, -0.0094, -0.0151,  0.0892,\n",
      "           0.1090,  0.1147, -0.1087, -0.0733, -0.0005,  0.1319, -0.1366,\n",
      "          -0.1210,  0.0094, -0.1123, -0.0509, -0.1379,  0.0388, -0.0994,\n",
      "           0.0027, -0.1276, -0.0286, -0.1184,  0.1325,  0.0168, -0.0963,\n",
      "          -0.1581, -0.1358, -0.0840,  0.0990, -0.0373, -0.0362, -0.1037,\n",
      "           0.0762, -0.0757,  0.0687,  0.0195, -0.1317,  0.1038, -0.0786,\n",
      "           0.0761, -0.0965, -0.1126,  0.1316]]], grad_fn=<ViewBackward0>) tensor(4.0940, grad_fn=<NllLossBackward0>)\n",
      "4 tensor([[[-0.0501,  0.1709,  0.0055,  0.0921, -0.1295,  0.1578, -0.0989,\n",
      "          -0.1387,  0.1157,  0.0610,  0.0008, -0.0384, -0.0455,  0.0720,\n",
      "           0.1248,  0.0999, -0.0930, -0.0331, -0.0025,  0.1274, -0.1101,\n",
      "          -0.1374,  0.0286, -0.0801, -0.0684, -0.1680,  0.0876, -0.0804,\n",
      "           0.0228, -0.0967, -0.0397, -0.1428,  0.1190,  0.0257, -0.0577,\n",
      "          -0.1556, -0.1383, -0.0665,  0.0885, -0.0497, -0.0198, -0.0972,\n",
      "           0.1006, -0.1032,  0.0752,  0.0077, -0.1338,  0.1087, -0.0374,\n",
      "           0.0761, -0.0704, -0.0764,  0.1121]]], grad_fn=<ViewBackward0>) tensor(3.8743, grad_fn=<NllLossBackward0>)\n",
      "5 tensor([[[-0.0477,  0.1245, -0.0003,  0.1105, -0.1084,  0.1198, -0.0763,\n",
      "          -0.1485,  0.0655,  0.1067, -0.0088, -0.0291, -0.0693,  0.0633,\n",
      "           0.1006,  0.0894, -0.1357, -0.0759, -0.0270,  0.1335, -0.1426,\n",
      "          -0.1317,  0.0289, -0.0823, -0.0426, -0.1445,  0.1098, -0.0767,\n",
      "          -0.0099, -0.0675, -0.0637, -0.1802,  0.1091,  0.0611, -0.1151,\n",
      "          -0.1527, -0.1548, -0.1079,  0.0565, -0.0168, -0.0424, -0.1053,\n",
      "           0.0710, -0.1152,  0.0768, -0.0185, -0.1468,  0.1119, -0.0618,\n",
      "           0.0748, -0.0765, -0.0523,  0.1076]]], grad_fn=<ViewBackward0>) tensor(4.0982, grad_fn=<NllLossBackward0>)\n",
      "6 tensor([[[-0.0250,  0.1367,  0.0230,  0.0891, -0.1164,  0.1357, -0.0834,\n",
      "          -0.1300,  0.0699,  0.1047,  0.0064, -0.0325, -0.0364,  0.0789,\n",
      "           0.1112,  0.0763, -0.1050, -0.1066, -0.0212,  0.1248, -0.1219,\n",
      "          -0.0927,  0.0241, -0.1065, -0.0288, -0.1408,  0.0975, -0.1139,\n",
      "          -0.0551, -0.0545, -0.0518, -0.1342,  0.1239,  0.0660, -0.1274,\n",
      "          -0.1334, -0.1673, -0.0988,  0.0492, -0.0098, -0.0417, -0.0870,\n",
      "           0.0597, -0.1038,  0.1051,  0.0033, -0.1130,  0.0788, -0.0676,\n",
      "           0.0595, -0.0639, -0.0417,  0.1057]]], grad_fn=<ViewBackward0>) tensor(3.8210, grad_fn=<NllLossBackward0>)\n",
      "7 tensor([[[-0.0338,  0.1451,  0.0037,  0.0826, -0.1137,  0.1486, -0.1163,\n",
      "          -0.1205,  0.0518,  0.0822,  0.0049, -0.0320,  0.0199,  0.1151,\n",
      "           0.1148,  0.0671, -0.1463, -0.1050, -0.0074,  0.1244, -0.1648,\n",
      "          -0.1167,  0.0160, -0.1369, -0.0088, -0.1765,  0.1078, -0.0963,\n",
      "          -0.0563, -0.0561, -0.0503, -0.1119,  0.1215,  0.0281, -0.1208,\n",
      "          -0.1472, -0.1559, -0.0988,  0.0697, -0.0453, -0.0181, -0.0330,\n",
      "           0.0730, -0.1130,  0.1084,  0.0234, -0.1197,  0.0752, -0.1031,\n",
      "           0.0490, -0.0442, -0.0757,  0.1060]]], grad_fn=<ViewBackward0>) tensor(3.8890, grad_fn=<NllLossBackward0>)\n",
      "8 tensor([[[-0.0245,  0.1547,  0.0042,  0.1051, -0.1120,  0.1327, -0.1236,\n",
      "          -0.1232,  0.0635,  0.0684, -0.0072, -0.0504,  0.0078,  0.1002,\n",
      "           0.1064,  0.0997, -0.1495, -0.0923, -0.0354,  0.1047, -0.1637,\n",
      "          -0.1061,  0.0564, -0.1216, -0.0204, -0.1622,  0.1207, -0.0955,\n",
      "          -0.0604, -0.0709, -0.0270, -0.1090,  0.1387,  0.0525, -0.0981,\n",
      "          -0.1620, -0.1677, -0.0930,  0.0753, -0.0383, -0.0403, -0.0709,\n",
      "           0.0903, -0.1022,  0.0756,  0.0467, -0.1306,  0.0710, -0.0831,\n",
      "           0.0330, -0.0742, -0.0386,  0.1224]]], grad_fn=<ViewBackward0>) tensor(3.8367, grad_fn=<NllLossBackward0>)\n",
      "9 tensor([[[-0.0305,  0.1092, -0.0108,  0.1117, -0.0984,  0.1077, -0.0864,\n",
      "          -0.1468,  0.0338,  0.1031, -0.0159, -0.0338, -0.0385,  0.0837,\n",
      "           0.0864,  0.0757, -0.1672, -0.1133, -0.0436,  0.1284, -0.1696,\n",
      "          -0.1112,  0.0460, -0.1089, -0.0093, -0.1458,  0.1208, -0.0838,\n",
      "          -0.0498, -0.0548, -0.0556, -0.1596,  0.1233,  0.0732, -0.1421,\n",
      "          -0.1579, -0.1685, -0.1166,  0.0497, -0.0039, -0.0507, -0.0944,\n",
      "           0.0638, -0.1172,  0.0820, -0.0019, -0.1441,  0.0962, -0.0957,\n",
      "           0.0510, -0.0788, -0.0354,  0.1121]]], grad_fn=<ViewBackward0>) tensor(4.0964, grad_fn=<NllLossBackward0>)\n",
      "10 tensor([[[-0.0143,  0.1240,  0.0140,  0.0848, -0.1103,  0.1292, -0.0860,\n",
      "          -0.1317,  0.0522,  0.0997, -0.0004, -0.0340, -0.0199,  0.0932,\n",
      "           0.1011,  0.0620, -0.1210, -0.1280, -0.0296,  0.1249, -0.1341,\n",
      "          -0.0802,  0.0348, -0.1244, -0.0067, -0.1435,  0.0982, -0.1185,\n",
      "          -0.0737, -0.0476, -0.0475, -0.1226,  0.1336,  0.0727, -0.1451,\n",
      "          -0.1365, -0.1721, -0.0994,  0.0447, -0.0005, -0.0448, -0.0842,\n",
      "           0.0556, -0.1050,  0.1106,  0.0101, -0.1110,  0.0731, -0.0893,\n",
      "           0.0457, -0.0657, -0.0345,  0.1079]]], grad_fn=<ViewBackward0>) tensor(4.0767, grad_fn=<NllLossBackward0>)\n",
      "11 tensor([[[ 0.0160,  0.1295,  0.0372,  0.0625, -0.1201,  0.1473, -0.0768,\n",
      "          -0.1021,  0.0678,  0.0955,  0.0026,  0.0077, -0.0321,  0.0928,\n",
      "           0.1110,  0.0639, -0.1464, -0.1170, -0.0085,  0.1004, -0.1393,\n",
      "          -0.0736,  0.0149, -0.1080, -0.0666, -0.1632,  0.1066, -0.1284,\n",
      "          -0.0644, -0.0511, -0.0484, -0.1195,  0.1523,  0.0656, -0.1052,\n",
      "          -0.1795, -0.1680, -0.0886,  0.0696, -0.0064, -0.0403, -0.0878,\n",
      "           0.0595, -0.1100,  0.0804,  0.0158, -0.1141,  0.0710, -0.0648,\n",
      "           0.0327, -0.0892, -0.0412,  0.1139]]], grad_fn=<ViewBackward0>) tensor(3.8272, grad_fn=<NllLossBackward0>)\n",
      "12 tensor([[[-1.3518e-02,  1.4871e-01,  6.3082e-03,  6.0461e-02, -1.1597e-01,\n",
      "           1.5039e-01, -1.1341e-01, -1.1331e-01,  4.4480e-02,  8.1725e-02,\n",
      "           3.6121e-03, -8.9064e-03,  1.5645e-02,  1.2480e-01,  1.1174e-01,\n",
      "           6.5804e-02, -1.6340e-01, -1.1421e-01, -1.1634e-04,  1.1139e-01,\n",
      "          -1.7567e-01, -1.0605e-01,  9.1848e-03, -1.4368e-01, -3.0315e-02,\n",
      "          -1.8461e-01,  1.0459e-01, -1.0726e-01, -6.2618e-02, -5.6317e-02,\n",
      "          -4.8142e-02, -1.0365e-01,  1.3275e-01,  2.4490e-02, -1.1436e-01,\n",
      "          -1.7171e-01, -1.5019e-01, -9.0414e-02,  8.3654e-02, -4.2005e-02,\n",
      "          -7.9972e-03, -3.2887e-02,  6.7827e-02, -1.2208e-01,  1.0084e-01,\n",
      "           2.9553e-02, -1.1459e-01,  6.5632e-02, -1.0555e-01,  3.6549e-02,\n",
      "          -5.7287e-02, -7.8440e-02,  1.1194e-01]]], grad_fn=<ViewBackward0>) tensor(4.0693, grad_fn=<NllLossBackward0>)\n",
      "13 tensor([[[ 0.0006,  0.1582,  0.0009,  0.0945, -0.0995,  0.1412, -0.1099,\n",
      "          -0.1576,  0.0684,  0.0618, -0.0238, -0.0152, -0.0092,  0.1255,\n",
      "           0.1088,  0.1098, -0.0902, -0.0922, -0.0404,  0.1394, -0.1093,\n",
      "          -0.1030, -0.0034, -0.0819, -0.0407, -0.1661,  0.0572, -0.0699,\n",
      "          -0.0447, -0.0872, -0.0607, -0.1084,  0.1319, -0.0081, -0.0980,\n",
      "          -0.1786, -0.1503, -0.0691,  0.1002, -0.0093, -0.0724, -0.0875,\n",
      "           0.0372, -0.0984,  0.1133,  0.0091, -0.1248,  0.0466, -0.0693,\n",
      "           0.0608, -0.0861, -0.0698,  0.1638]]], grad_fn=<ViewBackward0>) tensor(3.8182, grad_fn=<NllLossBackward0>)\n",
      "14 tensor([[[ 0.0061,  0.1597,  0.0217,  0.0759, -0.0737,  0.1309, -0.1181,\n",
      "          -0.1815,  0.0783,  0.0678, -0.0628, -0.0414, -0.0149,  0.1111,\n",
      "           0.0919,  0.0803, -0.1066, -0.0991, -0.0384,  0.0982, -0.1290,\n",
      "          -0.1059,  0.0207, -0.1464, -0.0443, -0.1860,  0.0900, -0.0975,\n",
      "          -0.0383, -0.1026, -0.0363, -0.1101,  0.1379, -0.0151, -0.1236,\n",
      "          -0.1980, -0.1638, -0.0701,  0.0589, -0.0517, -0.0685, -0.1073,\n",
      "           0.0527, -0.1164,  0.0774,  0.0447, -0.1586,  0.0792, -0.0886,\n",
      "           0.0786, -0.0689, -0.0647,  0.1458]]], grad_fn=<ViewBackward0>) tensor(4.0791, grad_fn=<NllLossBackward0>)\n",
      "15 tensor([[[-0.0421,  0.1737,  0.0141,  0.0887, -0.1095,  0.1513, -0.1157,\n",
      "          -0.1600,  0.1152,  0.0572, -0.0296, -0.0478, -0.0434,  0.0880,\n",
      "           0.1160,  0.0808, -0.0939, -0.0473, -0.0248,  0.1108, -0.1110,\n",
      "          -0.1229,  0.0376, -0.1010, -0.0600, -0.1939,  0.1150, -0.0753,\n",
      "           0.0050, -0.0829, -0.0401, -0.1303,  0.1183,  0.0095, -0.0729,\n",
      "          -0.1740, -0.1475, -0.0614,  0.0655, -0.0577, -0.0367, -0.0961,\n",
      "           0.0888, -0.1210,  0.0753,  0.0175, -0.1413,  0.0960, -0.0408,\n",
      "           0.0743, -0.0628, -0.0497,  0.1203]]], grad_fn=<ViewBackward0>) tensor(4.0593, grad_fn=<NllLossBackward0>)\n",
      "16 tensor([[[-0.0133,  0.1184,  0.0243,  0.0641, -0.1346,  0.1268, -0.0795,\n",
      "          -0.1577,  0.0890,  0.0761, -0.0176, -0.0074, -0.0506,  0.0893,\n",
      "           0.1060,  0.0993, -0.1045, -0.0407, -0.0484,  0.1053, -0.1309,\n",
      "          -0.1141, -0.0077, -0.1254, -0.0673, -0.1732,  0.1155, -0.1007,\n",
      "          -0.0373, -0.0742,  0.0146, -0.1389,  0.0808,  0.0420, -0.0919,\n",
      "          -0.1833, -0.1427, -0.0839,  0.0581, -0.0481, -0.0555, -0.0652,\n",
      "           0.1232, -0.1129,  0.0615, -0.0020, -0.0941,  0.0945, -0.0115,\n",
      "           0.0570, -0.0803,  0.0200,  0.0846]]], grad_fn=<ViewBackward0>) tensor(4.0610, grad_fn=<NllLossBackward0>)\n",
      "17 tensor([[[ 0.0080,  0.1444,  0.0431,  0.0578, -0.1372,  0.1455, -0.0781,\n",
      "          -0.1119,  0.0858,  0.0929, -0.0087,  0.0221, -0.0478,  0.0867,\n",
      "           0.1149,  0.0933, -0.1281, -0.0735, -0.0175,  0.0872, -0.1360,\n",
      "          -0.0895, -0.0078, -0.1027, -0.0994, -0.1684,  0.1116, -0.1152,\n",
      "          -0.0392, -0.0684, -0.0208, -0.1256,  0.1189,  0.0487, -0.0732,\n",
      "          -0.1937, -0.1550, -0.0837,  0.0812, -0.0256, -0.0440, -0.0775,\n",
      "           0.0866, -0.1206,  0.0543,  0.0078, -0.1038,  0.0755, -0.0154,\n",
      "           0.0389, -0.1014, -0.0166,  0.1043]]], grad_fn=<ViewBackward0>) tensor(3.8135, grad_fn=<NllLossBackward0>)\n",
      "18 tensor([[[-0.0218,  0.1657,  0.0099,  0.0628, -0.1269,  0.1476, -0.1176,\n",
      "          -0.1172,  0.0528,  0.0859, -0.0032, -0.0008,  0.0087,  0.1182,\n",
      "           0.1132,  0.0869, -0.1494, -0.0931, -0.0062,  0.1040, -0.1744,\n",
      "          -0.1152, -0.0028, -0.1374, -0.0480, -0.1822,  0.1074, -0.0975,\n",
      "          -0.0468, -0.0682, -0.0363, -0.1062,  0.1115,  0.0152, -0.0951,\n",
      "          -0.1748, -0.1464, -0.0921,  0.0907, -0.0492, -0.0096, -0.0276,\n",
      "           0.0778, -0.1306,  0.0882,  0.0247, -0.1111,  0.0638, -0.0759,\n",
      "           0.0416, -0.0629, -0.0667,  0.1082]]], grad_fn=<ViewBackward0>) tensor(3.9920, grad_fn=<NllLossBackward0>)\n",
      "19 tensor([[[ 1.9806e-02,  1.9699e-01,  5.7688e-03,  4.8946e-02, -1.3129e-01,\n",
      "           1.3372e-01, -9.8372e-02, -1.3548e-01,  6.0531e-02,  1.0116e-01,\n",
      "          -1.9872e-02, -1.2669e-02,  2.9273e-02,  8.8748e-02,  1.5121e-01,\n",
      "           1.0601e-01, -1.2979e-01, -7.0692e-02, -3.5038e-02,  1.5294e-01,\n",
      "          -1.8810e-01, -1.1816e-01, -1.4840e-02, -1.1813e-01, -4.6591e-02,\n",
      "          -1.7029e-01,  9.4004e-02, -1.2416e-01, -4.6762e-02, -7.5526e-02,\n",
      "          -1.6416e-02, -1.1202e-01,  7.8272e-02,  1.9321e-02, -1.2191e-01,\n",
      "          -2.0445e-01, -1.1118e-01, -8.5585e-02,  7.3612e-02, -1.8224e-03,\n",
      "          -2.9523e-02,  4.2988e-03,  8.8740e-02, -1.2981e-01,  9.1521e-02,\n",
      "           9.8571e-06, -1.2978e-01,  3.4323e-02, -6.9036e-02,  7.9502e-02,\n",
      "          -7.7723e-02, -5.6375e-02,  1.4247e-01]]], grad_fn=<ViewBackward0>) tensor(3.9293, grad_fn=<NllLossBackward0>)\n",
      "20 tensor([[[-0.0060,  0.1971, -0.0062,  0.0685, -0.1103,  0.1586, -0.0695,\n",
      "          -0.1416,  0.0816,  0.0173, -0.0680, -0.0449,  0.0315,  0.0813,\n",
      "           0.1267,  0.1048, -0.0754, -0.0825, -0.0335,  0.1691, -0.1546,\n",
      "          -0.0986, -0.0003, -0.0984, -0.0470, -0.1642,  0.0914, -0.1268,\n",
      "          -0.0377, -0.0673, -0.0245, -0.1142,  0.0502,  0.0281, -0.1277,\n",
      "          -0.1853, -0.1653, -0.0745,  0.0430, -0.0170, -0.0876, -0.0464,\n",
      "           0.1025, -0.1075,  0.0773, -0.0596, -0.1148,  0.0474, -0.0685,\n",
      "           0.0761, -0.0825, -0.0573,  0.1336]]], grad_fn=<ViewBackward0>) tensor(4.1197, grad_fn=<NllLossBackward0>)\n",
      "21 tensor([[[-0.0204,  0.1961,  0.0070,  0.1143, -0.1097,  0.1457, -0.0719,\n",
      "          -0.1254,  0.0816, -0.0175, -0.0180, -0.0333, -0.0033,  0.0994,\n",
      "           0.1364,  0.1245, -0.0500, -0.0921, -0.0446,  0.1650, -0.1160,\n",
      "          -0.1307,  0.0074, -0.0854, -0.0564, -0.1165,  0.1119, -0.1248,\n",
      "          -0.1127, -0.0781, -0.0158, -0.1180,  0.0933,  0.0502, -0.0969,\n",
      "          -0.1667, -0.1806, -0.0972,  0.0624, -0.0669, -0.0457, -0.0514,\n",
      "           0.1213, -0.1113,  0.0596, -0.0518, -0.0983,  0.0531, -0.0917,\n",
      "           0.0451, -0.0746, -0.0410,  0.0654]]], grad_fn=<ViewBackward0>) tensor(4.0723, grad_fn=<NllLossBackward0>)\n",
      "22 tensor([[[-0.0508,  0.2046,  0.0098,  0.0998, -0.1175,  0.1592, -0.0898,\n",
      "          -0.1366,  0.1168,  0.0195,  0.0008, -0.0428, -0.0299,  0.0751,\n",
      "           0.1417,  0.1009, -0.0742, -0.0369, -0.0192,  0.1452, -0.1111,\n",
      "          -0.1357,  0.0363, -0.0609, -0.0646, -0.1559,  0.1219, -0.0934,\n",
      "          -0.0291, -0.0707, -0.0300, -0.1325,  0.0983,  0.0386, -0.0578,\n",
      "          -0.1572, -0.1460, -0.0675,  0.0733, -0.0636, -0.0204, -0.0726,\n",
      "           0.1197, -0.1142,  0.0716, -0.0283, -0.1172,  0.0815, -0.0457,\n",
      "           0.0563, -0.0614, -0.0465,  0.0741]]], grad_fn=<ViewBackward0>) tensor(4.1186, grad_fn=<NllLossBackward0>)\n",
      "23 tensor([[[-0.0290,  0.1839,  0.0355,  0.0864, -0.1219,  0.1508, -0.0908,\n",
      "          -0.1259,  0.0955,  0.0699,  0.0199, -0.0338, -0.0213,  0.0796,\n",
      "           0.1241,  0.0832, -0.0799, -0.0843, -0.0123,  0.1299, -0.1137,\n",
      "          -0.0904,  0.0300, -0.0874, -0.0408, -0.1416,  0.1064, -0.1222,\n",
      "          -0.0655, -0.0567, -0.0296, -0.1082,  0.1243,  0.0578, -0.0974,\n",
      "          -0.1316, -0.1594, -0.0797,  0.0670, -0.0293, -0.0275, -0.0733,\n",
      "           0.0873, -0.1077,  0.1038, -0.0039, -0.1003,  0.0599, -0.0602,\n",
      "           0.0519, -0.0594, -0.0435,  0.0819]]], grad_fn=<ViewBackward0>) tensor(3.9753, grad_fn=<NllLossBackward0>)\n",
      "24 tensor([[[-0.0242,  0.1399, -0.0219,  0.0435, -0.1611,  0.1351, -0.1108,\n",
      "          -0.1116,  0.1012,  0.0898,  0.0013, -0.0292, -0.0349,  0.0907,\n",
      "           0.1155,  0.0906, -0.1090, -0.0703,  0.0061,  0.1486, -0.0981,\n",
      "          -0.1070,  0.0233, -0.0982, -0.0441, -0.1696,  0.0892, -0.1405,\n",
      "          -0.0371, -0.0628, -0.0555, -0.1174,  0.0824,  0.0549, -0.1206,\n",
      "          -0.1719, -0.1142, -0.0892,  0.0978, -0.0320, -0.0457, -0.0667,\n",
      "           0.0790, -0.1338,  0.1121, -0.0169, -0.1033,  0.0574, -0.0714,\n",
      "           0.0514, -0.0690, -0.0443,  0.1114]]], grad_fn=<ViewBackward0>) tensor(3.8165, grad_fn=<NllLossBackward0>)\n",
      "25 tensor([[[-0.0427,  0.1579, -0.0152,  0.0551, -0.1345,  0.1451, -0.1289,\n",
      "          -0.1070,  0.0697,  0.0817,  0.0027, -0.0314,  0.0186,  0.1224,\n",
      "           0.1179,  0.0788, -0.1455, -0.0848,  0.0080,  0.1336, -0.1509,\n",
      "          -0.1274,  0.0192, -0.1304, -0.0209, -0.1859,  0.1020, -0.1138,\n",
      "          -0.0442, -0.0627, -0.0546, -0.1021,  0.0983,  0.0194, -0.1159,\n",
      "          -0.1604, -0.1250, -0.0942,  0.0982, -0.0554, -0.0152, -0.0252,\n",
      "           0.0853, -0.1277,  0.1092,  0.0173, -0.1169,  0.0602, -0.1045,\n",
      "           0.0456, -0.0497, -0.0860,  0.1118]]], grad_fn=<ViewBackward0>) tensor(3.9008, grad_fn=<NllLossBackward0>)\n",
      "26 tensor([[[-4.3300e-02,  1.6446e-01,  2.0213e-04,  1.0033e-01, -1.2125e-01,\n",
      "           1.4044e-01, -1.1020e-01, -1.5849e-01,  9.9767e-02,  1.0701e-01,\n",
      "           6.4557e-05, -6.6644e-02, -1.8523e-02,  1.0168e-01,  1.3817e-01,\n",
      "           8.0860e-02, -1.3382e-01, -9.9747e-02,  1.0176e-02,  1.3392e-01,\n",
      "          -1.0135e-01, -1.2188e-01,  2.4074e-02, -1.0573e-01, -3.8451e-02,\n",
      "          -1.7032e-01,  5.8226e-02, -7.5808e-02,  1.3293e-03, -9.7596e-02,\n",
      "          -6.8404e-02, -8.5636e-02,  1.2021e-01, -2.9010e-02, -9.1959e-02,\n",
      "          -1.7115e-01, -8.8923e-02, -4.2419e-02,  1.1950e-01, -3.4674e-02,\n",
      "          -3.0775e-02, -8.3935e-02,  9.2315e-02, -1.0724e-01,  5.8424e-02,\n",
      "           2.6174e-03, -1.0769e-01,  9.5997e-02, -8.0330e-02,  4.6864e-02,\n",
      "          -1.1100e-01, -9.3643e-02,  1.1470e-01]]], grad_fn=<ViewBackward0>) tensor(3.9481, grad_fn=<NllLossBackward0>)\n",
      "27 tensor([[[-0.0385,  0.1308, -0.0345,  0.0572, -0.1578,  0.1251, -0.1177,\n",
      "          -0.1232,  0.1098,  0.1091, -0.0094, -0.0471, -0.0371,  0.1065,\n",
      "           0.1235,  0.0941, -0.1313, -0.0744,  0.0109,  0.1516, -0.0886,\n",
      "          -0.1248,  0.0249, -0.1095, -0.0401, -0.1830,  0.0682, -0.1209,\n",
      "          -0.0043, -0.0855, -0.0722, -0.1115,  0.0833,  0.0118, -0.1168,\n",
      "          -0.1895, -0.0864, -0.0731,  0.1143, -0.0367, -0.0474, -0.0702,\n",
      "           0.0844, -0.1289,  0.0825, -0.0164, -0.1079,  0.0843, -0.0769,\n",
      "           0.0490, -0.0945, -0.0680,  0.1319]]], grad_fn=<ViewBackward0>) tensor(3.8864, grad_fn=<NllLossBackward0>)\n",
      "28 tensor([[[-0.0471,  0.1031, -0.0208,  0.0849, -0.1155,  0.0988, -0.0819,\n",
      "          -0.1354,  0.0651,  0.1265, -0.0159, -0.0319, -0.0588,  0.0903,\n",
      "           0.1043,  0.0830, -0.1557, -0.0930, -0.0242,  0.1453, -0.1318,\n",
      "          -0.1291,  0.0329, -0.1011, -0.0262, -0.1518,  0.0969, -0.1012,\n",
      "          -0.0208, -0.0636, -0.0825, -0.1602,  0.0900,  0.0485, -0.1490,\n",
      "          -0.1670, -0.1236, -0.1087,  0.0677, -0.0129, -0.0514, -0.0906,\n",
      "           0.0658, -0.1174,  0.0760, -0.0282, -0.1331,  0.1055, -0.0865,\n",
      "           0.0578, -0.0915, -0.0560,  0.1204]]], grad_fn=<ViewBackward0>) tensor(3.8532, grad_fn=<NllLossBackward0>)\n",
      "29 tensor([[[-0.0455,  0.0844, -0.0202,  0.0957, -0.0926,  0.0896, -0.0637,\n",
      "          -0.1443,  0.0390,  0.1293, -0.0234, -0.0282, -0.0628,  0.0809,\n",
      "           0.0991,  0.0776, -0.1646, -0.1054, -0.0439,  0.1424, -0.1500,\n",
      "          -0.1320,  0.0324, -0.1017, -0.0159, -0.1396,  0.1071, -0.0889,\n",
      "          -0.0300, -0.0513, -0.0888, -0.1853,  0.0900,  0.0666, -0.1646,\n",
      "          -0.1586, -0.1448, -0.1223,  0.0389, -0.0042, -0.0589, -0.1016,\n",
      "           0.0521, -0.1100,  0.0753, -0.0340, -0.1423,  0.1157, -0.0890,\n",
      "           0.0611, -0.0872, -0.0428,  0.1163]]], grad_fn=<ViewBackward0>) tensor(3.8699, grad_fn=<NllLossBackward0>)\n",
      "30 tensor([[[-0.0352,  0.1259, -0.0156,  0.1014, -0.0997,  0.1063, -0.0950,\n",
      "          -0.1278,  0.0543,  0.0902, -0.0202, -0.0531, -0.0255,  0.0814,\n",
      "           0.1141,  0.1144, -0.1513, -0.0885, -0.0540,  0.1083, -0.1473,\n",
      "          -0.1215,  0.0614, -0.1043, -0.0280, -0.1408,  0.1113, -0.0942,\n",
      "          -0.0455, -0.0699, -0.0495, -0.1462,  0.1158,  0.0737, -0.1148,\n",
      "          -0.1605, -0.1591, -0.1016,  0.0534, -0.0340, -0.0590, -0.1007,\n",
      "           0.0803, -0.0909,  0.0560,  0.0176, -0.1327,  0.0904, -0.0720,\n",
      "           0.0357, -0.0912, -0.0190,  0.1312]]], grad_fn=<ViewBackward0>) tensor(3.8275, grad_fn=<NllLossBackward0>)\n",
      "31 tensor([[[-0.0437,  0.1455, -0.0234,  0.0847, -0.1065,  0.1339, -0.1216,\n",
      "          -0.1192,  0.0400,  0.0792, -0.0078, -0.0454,  0.0257,  0.1139,\n",
      "           0.1155,  0.0847, -0.1652, -0.1024, -0.0212,  0.1188, -0.1748,\n",
      "          -0.1300,  0.0332, -0.1356, -0.0071, -0.1746,  0.1058, -0.0879,\n",
      "          -0.0486, -0.0683, -0.0489, -0.1175,  0.1217,  0.0322, -0.1125,\n",
      "          -0.1560, -0.1541, -0.0978,  0.0716, -0.0580, -0.0230, -0.0407,\n",
      "           0.0825, -0.1119,  0.0856,  0.0274, -0.1263,  0.0795, -0.1064,\n",
      "           0.0352, -0.0554, -0.0657,  0.1199]]], grad_fn=<ViewBackward0>) tensor(3.8690, grad_fn=<NllLossBackward0>)\n",
      "32 tensor([[[-0.0322,  0.1573, -0.0123,  0.1053, -0.1091,  0.1252, -0.1249,\n",
      "          -0.1218,  0.0563,  0.0695, -0.0134, -0.0589,  0.0109,  0.0985,\n",
      "           0.1065,  0.1081, -0.1568, -0.0933, -0.0414,  0.1041, -0.1687,\n",
      "          -0.1124,  0.0649, -0.1198, -0.0190, -0.1598,  0.1157, -0.0920,\n",
      "          -0.0556, -0.0796, -0.0266, -0.1122,  0.1402,  0.0548, -0.0928,\n",
      "          -0.1636, -0.1676, -0.0920,  0.0755, -0.0459, -0.0408, -0.0743,\n",
      "           0.0950, -0.1037,  0.0647,  0.0464, -0.1332,  0.0738, -0.0862,\n",
      "           0.0252, -0.0777, -0.0355,  0.1294]]], grad_fn=<ViewBackward0>) tensor(3.8404, grad_fn=<NllLossBackward0>)\n",
      "33 tensor([[[-0.0356,  0.1111, -0.0200,  0.1117, -0.0973,  0.1037, -0.0862,\n",
      "          -0.1456,  0.0300,  0.1059, -0.0183, -0.0389, -0.0369,  0.0818,\n",
      "           0.0859,  0.0794, -0.1704, -0.1150, -0.0462,  0.1299, -0.1728,\n",
      "          -0.1136,  0.0507, -0.1071, -0.0086, -0.1438,  0.1161, -0.0828,\n",
      "          -0.0473, -0.0610, -0.0553, -0.1611,  0.1258,  0.0746, -0.1390,\n",
      "          -0.1575, -0.1686, -0.1160,  0.0493, -0.0084, -0.0498, -0.0957,\n",
      "           0.0658, -0.1194,  0.0768, -0.0030, -0.1454,  0.0984, -0.0984,\n",
      "           0.0467, -0.0795, -0.0352,  0.1158]]], grad_fn=<ViewBackward0>) tensor(3.9961, grad_fn=<NllLossBackward0>)\n",
      "34 tensor([[[-0.0295,  0.1030, -0.0580,  0.0517, -0.1456,  0.1152, -0.1065,\n",
      "          -0.1199,  0.0663,  0.1048, -0.0271, -0.0383, -0.0400,  0.0947,\n",
      "           0.1018,  0.0849, -0.1452, -0.0917, -0.0145,  0.1516, -0.1203,\n",
      "          -0.1204,  0.0350, -0.1175, -0.0230, -0.1717,  0.0861, -0.1230,\n",
      "          -0.0239, -0.0666, -0.0727, -0.1456,  0.0816,  0.0624, -0.1415,\n",
      "          -0.1848, -0.1202, -0.1007,  0.0787, -0.0244, -0.0588, -0.0793,\n",
      "           0.0633, -0.1388,  0.0983, -0.0153, -0.1240,  0.0806, -0.0902,\n",
      "           0.0455, -0.0750, -0.0383,  0.1348]]], grad_fn=<ViewBackward0>) tensor(3.8975, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(2, len(first_clue)):\n",
    "    output, hidden = model(target, hidden)\n",
    "    target = torch.Tensor([vocab_stoi[first_clue[i]]]).long().unsqueeze(0)\n",
    "    loss = criterion(output.reshape(-1, vocab_size), # reshape to 2D tensor\n",
    "                     target.reshape(-1))             # reshape to 1D tensor\n",
    "    print(i, output, loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we'd expected the answer length token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<3>'"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_token = f'<{len(first_answer)}>'\n",
    "answer_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 tensor([[[-0.0425,  0.1387, -0.0277,  0.0906, -0.1266,  0.1276, -0.1000,\n",
      "          -0.1590,  0.0964,  0.1226, -0.0134, -0.0744, -0.0434,  0.0870,\n",
      "           0.1391,  0.0849, -0.1287, -0.1005,  0.0016,  0.1386, -0.0804,\n",
      "          -0.1239,  0.0313, -0.0982, -0.0442, -0.1599,  0.0415, -0.0826,\n",
      "           0.0145, -0.1008, -0.0824, -0.1046,  0.1095, -0.0065, -0.1026,\n",
      "          -0.1756, -0.0787, -0.0439,  0.1088, -0.0291, -0.0454, -0.1067,\n",
      "           0.0831, -0.1080,  0.0511, -0.0123, -0.1071,  0.1047, -0.0739,\n",
      "           0.0446, -0.1206, -0.0734,  0.1319]]], grad_fn=<ViewBackward0>) tensor(3.8722, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output, hidden = model(target, hidden)\n",
    "target = torch.Tensor([vocab_stoi[answer_token]]).long().unsqueeze(0)\n",
    "loss = criterion(output.reshape(-1, vocab_size), # reshape to 2D tensor\n",
    "                 target.reshape(-1))             # reshape to 1D tensor\n",
    "print(i, output, loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, on to the answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor([[[-0.0544,  0.1547, -0.0541,  0.1441, -0.1214,  0.1471, -0.1003,\n",
      "          -0.1201,  0.0877,  0.0842, -0.0173, -0.0832, -0.0372,  0.0544,\n",
      "           0.1741,  0.0905, -0.1312, -0.0980, -0.0154,  0.1541, -0.0847,\n",
      "          -0.1468,  0.0150, -0.0603, -0.0431, -0.1722,  0.0678, -0.0733,\n",
      "          -0.0132, -0.1018, -0.0878, -0.1071,  0.1158, -0.0248, -0.0991,\n",
      "          -0.1741, -0.1290, -0.0874,  0.0828,  0.0061, -0.0631, -0.0697,\n",
      "           0.1006, -0.0706,  0.0327, -0.0043, -0.0982,  0.1172, -0.0567,\n",
      "           0.0412, -0.1139, -0.0592,  0.1274]]], grad_fn=<ViewBackward0>) tensor(4.0298, grad_fn=<NllLossBackward0>)\n",
      "1 tensor([[[-1.7521e-02,  1.7617e-01, -2.4879e-02,  1.2930e-01, -9.3419e-02,\n",
      "           1.5921e-01, -1.1288e-01, -8.4005e-02,  1.1554e-01,  5.4016e-02,\n",
      "          -3.3791e-03, -6.7495e-02, -9.4511e-03,  4.5579e-02,  1.6342e-01,\n",
      "           6.2786e-02, -1.6795e-01, -8.7427e-02, -1.0543e-04,  1.5991e-01,\n",
      "          -1.2854e-01, -1.0165e-01,  5.7618e-03, -6.0862e-02, -7.8996e-02,\n",
      "          -1.6621e-01,  5.7451e-02, -1.0111e-01, -2.0348e-02, -1.2393e-01,\n",
      "          -8.9408e-02, -9.5913e-02,  1.2700e-01, -2.1380e-02, -1.0261e-01,\n",
      "          -1.9305e-01, -1.0999e-01, -9.7355e-02,  8.4246e-02, -1.0577e-02,\n",
      "          -5.2452e-02, -5.9376e-02,  9.4622e-02, -1.1924e-01,  3.8773e-02,\n",
      "           1.0269e-02, -9.1923e-02,  9.5160e-02, -6.0659e-02,  2.1567e-03,\n",
      "          -1.4025e-01, -8.1809e-02,  1.1150e-01]]], grad_fn=<ViewBackward0>) tensor(3.9640, grad_fn=<NllLossBackward0>)\n",
      "2 tensor([[[-0.0228,  0.1898, -0.0228,  0.0991, -0.0902,  0.1679, -0.0727,\n",
      "          -0.1205,  0.1105,  0.0062, -0.0513, -0.0651,  0.0039,  0.0577,\n",
      "           0.1321,  0.0852, -0.0984, -0.0844, -0.0126,  0.1660, -0.1289,\n",
      "          -0.0905,  0.0140, -0.0681, -0.0635, -0.1615,  0.0666, -0.1210,\n",
      "          -0.0255, -0.0907, -0.0570, -0.1126,  0.0861,  0.0123, -0.1240,\n",
      "          -0.1810, -0.1589, -0.0856,  0.0513, -0.0208, -0.0859, -0.0733,\n",
      "           0.1034, -0.1003,  0.0520, -0.0544, -0.0961,  0.0825, -0.0615,\n",
      "           0.0391, -0.1121, -0.0712,  0.1174]]], grad_fn=<ViewBackward0>) tensor(4.0657, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(first_answer)):\n",
    "    output, hidden = model(target, hidden)\n",
    "    target = torch.Tensor([vocab_stoi[first_answer[i]]]).long().unsqueeze(0)\n",
    "    loss = criterion(output.reshape(-1, vocab_size), # reshape to 2D tensor\n",
    "                     target.reshape(-1))             # reshape to 1D tensor\n",
    "    print(i, output, loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we'd expect the `<EOA>` token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 tensor([[[-0.0013,  0.1725, -0.0017,  0.0967, -0.1000,  0.1498, -0.0957,\n",
      "          -0.1276,  0.0858,  0.0337, -0.0220, -0.0270, -0.0096,  0.0711,\n",
      "           0.1131,  0.0948, -0.1101, -0.0705, -0.0088,  0.1455, -0.1491,\n",
      "          -0.1231, -0.0018, -0.0962, -0.0506, -0.1395,  0.0421, -0.0946,\n",
      "          -0.0020, -0.1256, -0.0534, -0.1088,  0.1242,  0.0180, -0.1075,\n",
      "          -0.1695, -0.1415, -0.0977,  0.0819, -0.0364, -0.0537, -0.0866,\n",
      "           0.0773, -0.0780,  0.0668, -0.0122, -0.1290,  0.1002, -0.0497,\n",
      "           0.0686, -0.0962, -0.1009,  0.1423]]], grad_fn=<ViewBackward0>) tensor(3.9923, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "output, hidden = model(target, hidden)\n",
    "target = torch.Tensor([vocab_stoi[\"<EOA>\"]]).long().unsqueeze(0)\n",
    "loss = criterion(output.reshape(-1, vocab_size), # reshape to 2D tensor\n",
    "                 target.reshape(-1))             # reshape to 1D tensor\n",
    "print(i, output, loss)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We don't actually need to loop like this though, we can just pass in the sequence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 41])\n"
     ]
    }
   ],
   "source": [
    "indices = [vocab_stoi[ch] for ch in token_batch[0]]\n",
    "tensor = torch.Tensor(indices).long().unsqueeze(0)\n",
    "\n",
    "print(tensor.shape)\n",
    "\n",
    "output, hidden = model(tensor[:,:-1]) # <EOS> is never an input token\n",
    "target = tensor[:,1:]                 # <BOS> is never a target token\n",
    "loss = criterion(output.reshape(-1, vocab_size), # reshape to 2D tensor\n",
    "                 target.reshape(-1))             # reshape to 1D tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter 100] Loss 2.502622\n",
      "[Iter 200] Loss 0.904687\n",
      "[Iter 300] Loss 0.151702\n",
      "[Iter 400] Loss 0.040548\n",
      "[Iter 500] Loss 0.019633\n"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "for it in range(500):\n",
    "    optimizer.zero_grad()\n",
    "    output, _ = model(tensor[:,:-1])\n",
    "    loss = criterion(output.reshape(-1, vocab_size),\n",
    "                 target.reshape(-1))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (it+1) % 100 == 0:\n",
    "        print(\"[Iter %d] Loss %f\" % (it+1, float(loss)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def build_tokens(row):\n",
    "    clue = row['clue']\n",
    "    answer = row['answer']\n",
    "    tokens = ['<BOC>']\n",
    "    tokens.extend(list(clue))\n",
    "    tokens.append('<EOC>')\n",
    "    tokens.append('<' + str(len(answer)) + '>')\n",
    "    tokens.extend(list(answer))\n",
    "    tokens.append('<EOA>')\n",
    "    return tokens\n",
    "\n",
    "token_batch = []\n",
    "for i in range(5000):\n",
    "    tokens = build_tokens(df.iloc[i])\n",
    "    token_batch.append(tokens)\n",
    "\n",
    "rows = df.iloc[:5000]\n",
    "all_chars = ''.join(rows['answer']) + ''.join(rows['clue'])\n",
    "\n",
    "vocab = sorted(list(set(all_chars)))\n",
    "vocab += ['<BOC>', '<EOC>', '<EOA>']\n",
    "\n",
    "max_answer_length = rows['answer'].str.len().max()\n",
    "for i in range(1, max_answer_length + 1):\n",
    "    vocab.append(f'<{i}>')\n",
    "\n",
    "vocab_stoi = {s:i for i,s in enumerate(vocab)}\n",
    "vocab_itos = {i:s for s,i in vocab_stoi.items()}\n",
    "vocab_size = len(vocab)\n",
    "vocab_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data, batch_size=1, num_epochs=1, lr=0.001, print_every=100):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    it = 0\n",
    "    \n",
    "    for e in range(num_epochs):\n",
    "        # get training set\n",
    "        avg_loss = 0\n",
    "\n",
    "        for token in data:\n",
    "            indicies = torch.tensor(list(map(lambda x: vocab_stoi[x], token)))\n",
    "            target = indicies[1:]\n",
    "            inp = indicies[:-1]\n",
    "            # target = token[:, 1:] # skip the first character as that's not a target\n",
    "            # inp = token[:, :-1] # skil the last value as it's never an input\n",
    "            # cleanup\n",
    "            optimizer.zero_grad()\n",
    "            # forward pass\n",
    "            output, _ = model(inp)\n",
    "            loss = criterion(output.reshape(-1, vocab_size), target.reshape(-1))\n",
    "            # backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_loss += loss\n",
    "            it += 1 # increment iteration count\n",
    "            if it % print_every == 0:\n",
    "                print(\"[Iter %d] Loss %f\" % (it+1, float(avg_loss/print_every)))\n",
    "                print(token)\n",
    "                #print(indicies.shape)\n",
    "                #print(output.shape)\n",
    "                predicted_indicies = output.argmax(1)\n",
    "                #print(predicted_indicies.shape)\n",
    "                print(list(map(lambda x: vocab_itos[x.item()], predicted_indicies)))\n",
    "                # get answer length, would be nice to have this beforehand\n",
    "                #print(\"    \" + sample_sequence(model, 140, 0.8))\n",
    "                avg_loss = 0\n",
    "\n",
    "model = TextGenerator(vocab_size, 128)\n",
    "train(model, token_batch, batch_size=1, num_epochs=1, lr=0.003, print_every=1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open Questions\n",
    "* Should we use separate vocabs for answers??\n",
    "\n",
    "## References\n",
    "* https://www.cs.toronto.edu/~lczhang/360/lec/w08/gen.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
