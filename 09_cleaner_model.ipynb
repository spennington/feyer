{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f7c2d9e-9c5d-4062-9616-6304d85b91ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PyTorch, a cleaner story\n",
    "\n",
    "In [07_pytorch_exploration.ipynb](07_pytorch_exploration.ipynb) we built our first network _actually_ using PyTorch, but it was pretty messy and hard to follow. Let's continue that work and see what else we can learn/optimize (both the human and the model ðŸ™ƒ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5d121da-9dbf-4b44-8720-fd519102d742",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "PADDING_TOKEN_INDEX = 0\n",
    "PAD_TO_SIZE = 45\n",
    "\n",
    "class CrosswordClueAnswersDataset(Dataset):\n",
    "    \"\"\"Crossword clues and answers dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with clues and answers.\n",
    "        \"\"\"\n",
    "        self.values = pd.read_csv(csv_file, keep_default_na=False).values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.values)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        data = self.values[idx, :]\n",
    "        return (data[0], data[1])\n",
    "\n",
    "class CrosswordModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, padding_size, hidden_size, num_class, device):\n",
    "        super(CrosswordModel, self).__init__()\n",
    "        self.C = torch.nn.Embedding(vocab_size, embed_dim, device=device)\n",
    "        self.W1 = torch.nn.Linear(embed_dim * padding_size, hidden_size, device=device)\n",
    "        self.M = nn.Tanh()\n",
    "        self.W2 = torch.nn.Linear(hidden_size, num_class, device=device)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.C.weight.data.uniform_(-initrange, initrange)\n",
    "        self.W1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.W1.bias.data.zero_()\n",
    "        self.W2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.W2.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text):\n",
    "        emb = self.C(text)\n",
    "        h = self.M(self.W1(emb.view(-1, self.W1.in_features)))\n",
    "        return self.W2(h)\n",
    "    \n",
    "class DataHandler():\n",
    "    \n",
    "    def __init__(self, device):\n",
    "        self.tokenizer = get_tokenizer('basic_english')\n",
    "        self.device = device\n",
    "\n",
    "    def yield_clues(self, data_iter):\n",
    "        for _, clue in data_iter:\n",
    "            yield self.tokenizer(clue)\n",
    "\n",
    "    def yield_answers(self, data_iter):\n",
    "        for answer, _ in data_iter:\n",
    "            yield self.tokenizer(answer)\n",
    "            \n",
    "    def clue_pipeline(self, x):\n",
    "        return self.clues_vocab(self.tokenizer(x))\n",
    "    \n",
    "    def answer_pipeline(self, x):\n",
    "        return self.answers_vocab([x])[0]\n",
    "        \n",
    "    def collate_batch(self, batch):\n",
    "        answer_list, clue_list = [], []\n",
    "\n",
    "        for (answer, clue) in batch:\n",
    "            clue_indicies = self.clue_pipeline(clue)\n",
    "            clue_indicies += [PADDING_TOKEN_INDEX] * (PAD_TO_SIZE - len(clue_indicies))\n",
    "            clue_list.append(clue_indicies)\n",
    "\n",
    "            answer_list.append(self.answer_pipeline(answer))\n",
    "\n",
    "        answer_list = torch.tensor(answer_list).to(device)\n",
    "        clue_list = torch.tensor(clue_list).to(device)\n",
    "\n",
    "        return answer_list, clue_list\n",
    "\n",
    "    def createDatasets(self):\n",
    "        \"\"\"\n",
    "        Creates train, test, and dev datasets.\n",
    "        Returns a tuple of (test, train, dev, all)\n",
    "        \"\"\"\n",
    "        self.dataset = CrosswordClueAnswersDataset(\"cleaned_data/clean_2.csv\")\n",
    "        train_size = int(0.8 * len(self.dataset))\n",
    "        dev_size = int(0.1 * len(self.dataset))\n",
    "        test_size = len(self.dataset) - train_size - dev_size\n",
    "        # this manual_seed is important to ensure that we consistently split the dataset\n",
    "        g = torch.Generator().manual_seed(42)\n",
    "        self.train_dataset, self.test_dataset, self.dev_dataset = torch.utils.data.random_split(self.dataset, [train_size, test_size, dev_size], generator=g)\n",
    "\n",
    "        return (self.train_dataset, self.test_dataset, self.dev_dataset, self.dataset)\n",
    "\n",
    "    def createVocabs(self):\n",
    "        \"\"\"\n",
    "        Creates answers and clues vocabularies\n",
    "        Returns tuple of (answers_vocab, clues_vocab)\n",
    "        \"\"\"\n",
    "        self.answers_vocab = build_vocab_from_iterator(self.yield_answers(train_dataset), specials=['<unk>'])\n",
    "        # add this in case we see an answer in the test/dev set that we don't have in the training set\n",
    "        self.answers_vocab.set_default_index(0)\n",
    "        self.clues_vocab = build_vocab_from_iterator(self.yield_clues(train_dataset), specials=['<pad>', '<unk>'])\n",
    "        self.clues_vocab.set_default_index(1)\n",
    "        return (self.answers_vocab, self.clues_vocab)\n",
    "\n",
    "def train(model, optimizer, criterion, dataloader, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    # stats for each log interval\n",
    "    log_interval_batch_count, log_interval_count, log_interval_loss, log_interval_acc,  = 0, 0, 0, 0\n",
    "    log_interval = 200\n",
    "    log_interval_start_time = time.time()\n",
    "    \n",
    "    # stats for the entire dataset\n",
    "    running_loss, running_acc, running_count = 0, 0, 0\n",
    "\n",
    "    for idx, (answer, clue) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_answer = model(clue)\n",
    "        loss = criterion(predicted_answer, answer)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_acc = (predicted_answer.argmax(1) == answer).sum().item()\n",
    "        batch_size = answer.size(0)\n",
    "        \n",
    "        running_count += batch_size\n",
    "        running_loss += loss.item()\n",
    "        running_acc += batch_acc\n",
    "\n",
    "        log_interval_batch_count += 1\n",
    "        log_interval_count += batch_size\n",
    "        log_interval_loss += loss.item() \n",
    "        log_interval_acc += batch_acc\n",
    "\n",
    "        \n",
    "        if idx % log_interval == 0:\n",
    "            elapsed = time.time() - log_interval_start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f} | loss {:8.7f} | time {:5.2f}s '.format(epoch, idx, len(dataloader),\n",
    "                                              log_interval_acc / log_interval_count, log_interval_loss / log_interval_batch_count, elapsed))\n",
    "            log_interval_batch_count, log_interval_count, log_interval_acc, log_interval_loss = 0, 0, 0, 0\n",
    "            log_interval_start_time = time.time()\n",
    "    \n",
    "    return running_acc / running_count, running_loss / len(dataloader)\n",
    "\n",
    "class Trainer:\n",
    "    def evaluate(self, model, criterion, dataloader):\n",
    "        \"\"\"\n",
    "        Evaluate the model against a dataset\n",
    "        Returns a tuple of (acurate_pct, loss)\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        total_acc, total_count, running_loss = 0, 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx, (answer, clue) in enumerate(dataloader):\n",
    "                predicted_answer = model(clue)\n",
    "                loss = criterion(predicted_answer, answer).item()\n",
    "                total_acc += (predicted_answer.argmax(1) == answer).sum().item()\n",
    "                total_count += answer.size(0)\n",
    "                running_loss += loss\n",
    "        return total_acc / total_count, running_loss / len(dataloader)\n",
    "\n",
    "    def start(self, hyperparameters, data_handler, previous_model):\n",
    "        \"\"\"\n",
    "        Creates a model and trains it based on the hyperparameters\n",
    "        Model output is saved\n",
    "        Returns accu_pct, loss\n",
    "        \"\"\"\n",
    "        # shuffle the training dataloader so we go through different batches each time\n",
    "        train_dataloader = DataLoader(data_handler.train_dataset, batch_size=hyperparameters['BATCH_SIZE'], shuffle=True, collate_fn=data_handler.collate_batch)\n",
    "        dev_dataloader = DataLoader(data_handler.dev_dataset, batch_size=hyperparameters['BATCH_SIZE'], shuffle=False, collate_fn=data_handler.collate_batch)\n",
    "\n",
    "        num_class = len(data_handler.answers_vocab)\n",
    "        vocab_size = len(data_handler.clues_vocab)\n",
    "        model = CrosswordModel(vocab_size, hyperparameters['EMBEDDING_LAYER_SIZE'], PAD_TO_SIZE, hyperparameters['HIDDEN_SIZE'], num_class, device)\n",
    "\n",
    "        criterion = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=hyperparameters['LR'])\n",
    "        \n",
    "        # load previous model if we have one\n",
    "        if previous_model is not None:\n",
    "            #TODO: would be nice to just continue this run - we'd need to save state for the optimizer, load up the training dict, and maybe some other stuff\n",
    "            print('Loading from previous model:', previous_model)\n",
    "            model.load_state_dict(torch.load(previous_model))\n",
    "            dev_accu_pct, dev_loss = self.evaluate(model, criterion, dev_dataloader)\n",
    "            print(f'Starting with model: {dev_accu_pct} accuracy; {dev_loss} loss;')\n",
    "        \n",
    "        # setup directories and files for output\n",
    "        timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        train_dir = os.path.join('training_results', timestr + '-training')\n",
    "        if not os.path.exists(train_dir):\n",
    "            os.makedirs(train_dir)\n",
    "        results_filename = os.path.join(train_dir, 'training-results.json')\n",
    "        \n",
    "        # stats to track for eeach epoch\n",
    "        dev_accu_pcts, dev_losses = [], []\n",
    "        train_accu_pcts, train_losses = [], []\n",
    "        lrs, elapsed_times = [], []\n",
    "\n",
    "        for epoch in range(1, hyperparameters['EPOCHS'] + 1):\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            train_accu_pct, train_loss = train(model, optimizer, criterion, train_dataloader, epoch)\n",
    "            train_accu_pcts.append(train_accu_pct)\n",
    "            train_losses.append(train_loss)\n",
    "            \n",
    "            dev_accu_pct, dev_loss = self.evaluate(model, criterion, dev_dataloader)\n",
    "            dev_accu_pcts.append(dev_accu_pct)\n",
    "            dev_losses.append(dev_loss)\n",
    "            # learning rate doesn't change for now. Maybe we should log some detail from the optimzer?\n",
    "            lrs.append(hyperparameters['LR'])\n",
    "            \n",
    "            epoch_elapsed_time = time.time() - epoch_start_time\n",
    "            elapsed_times.append(epoch_elapsed_time)\n",
    "            \n",
    "            print('-' * 59)\n",
    "            print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "                  'dev accuracy {:8.3f} | loss {:8.7f} '.format(epoch,\n",
    "                                                   epoch_elapsed_time,\n",
    "                                                   dev_accu_pct, dev_loss))\n",
    "            print('-' * 59)\n",
    "            \n",
    "            \n",
    "            # save results and model for this epoch\n",
    "            model_filename = os.path.join(train_dir, 'model-epoch-' + str(epoch) + '.pt')\n",
    "            training_dict = {\n",
    "                'hyperparameters': hyperparameters,\n",
    "                'dev_accu_pcts': dev_accu_pcts,\n",
    "                'dev_losses': dev_losses,\n",
    "                'train_accu_pcts': train_accu_pcts,\n",
    "                'train_losses': train_losses,\n",
    "                'elapsed_times': elapsed_times,\n",
    "                'learning_rates': lrs,\n",
    "                'model': model_filename,\n",
    "                'previous_model': previous_model\n",
    "            }\n",
    "\n",
    "            with open(results_filename, 'w') as file:\n",
    "                 file.write(json.dumps(training_dict))\n",
    "            torch.save(model.state_dict(), model_filename)\n",
    "\n",
    "        return model, dev_accu_pcts, dev_losses, train_accu_pcts, train_losses, elapsed_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e689731e-b5eb-48a4-9606-ef96c6d6051b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: mps\n",
      "len(dataset)=770361\n",
      "len(train_dataset)=616288\n",
      "len(test_dataset)=77037\n",
      "len(dev_dataset)=77036\n",
      "\n",
      "len(answers_vocab)=60323,len(clues_vocab)=77891\n",
      "\n",
      "CPU times: user 4.2 s, sys: 68.5 ms, total: 4.27 s\n",
      "Wall time: 4.27 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# attempt to run on mps - will do work on the GPU for MacOS\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "#device = 'cpu'\n",
    "print(f'Running on:', device)\n",
    "\n",
    "data_handler = DataHandler(device)\n",
    "\n",
    "# load dataset, split data\n",
    "train_dataset, test_dataset, dev_dataset, dataset = data_handler.createDatasets()\n",
    "print(f'{len(dataset)=}\\n{len(train_dataset)=}\\n{len(test_dataset)=}\\n{len(dev_dataset)=}\\n')\n",
    "\n",
    "# build vocabulary\n",
    "answers_vocab, clues_vocab = data_handler.createVocabs()\n",
    "\n",
    "print(f'{len(answers_vocab)=},{len(clues_vocab)=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8f0ef2e-4a31-44ab-9580-b5ed5447fa1b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EPOCHS': 25, 'LR': 0.001, 'BATCH_SIZE': 256, 'EMBEDDING_LAYER_SIZE': 50, 'HIDDEN_SIZE': 100}\n",
      "| epoch   1 |     0/ 2408 batches | accuracy    0.000 | loss 14.1939583 | time  0.19s \n",
      "| epoch   1 |   200/ 2408 batches | accuracy    0.000 | loss 12.4046675 | time 18.03s \n",
      "| epoch   1 |   400/ 2408 batches | accuracy    0.000 | loss 11.5938124 | time 18.03s \n",
      "| epoch   1 |   600/ 2408 batches | accuracy    0.002 | loss 11.1976735 | time 18.07s \n",
      "| epoch   1 |   800/ 2408 batches | accuracy    0.002 | loss 10.9545483 | time 18.10s \n",
      "| epoch   1 |  1000/ 2408 batches | accuracy    0.005 | loss 10.7679338 | time 18.03s \n",
      "| epoch   1 |  1200/ 2408 batches | accuracy    0.007 | loss 10.6005633 | time 18.05s \n",
      "| epoch   1 |  1400/ 2408 batches | accuracy    0.012 | loss 10.4701159 | time 18.02s \n",
      "| epoch   1 |  1600/ 2408 batches | accuracy    0.015 | loss 10.3548116 | time 18.01s \n",
      "| epoch   1 |  1800/ 2408 batches | accuracy    0.021 | loss 10.2325546 | time 18.05s \n",
      "| epoch   1 |  2000/ 2408 batches | accuracy    0.026 | loss 10.1217430 | time 17.99s \n",
      "| epoch   1 |  2200/ 2408 batches | accuracy    0.032 | loss 9.9770449 | time 18.02s \n",
      "| epoch   1 |  2400/ 2408 batches | accuracy    0.038 | loss 9.8500357 | time 18.04s \n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 219.69s | dev accuracy    0.041 | loss 9.6459994 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |     0/ 2408 batches | accuracy    0.059 | loss 8.9743176 | time  0.11s \n",
      "| epoch   2 |   200/ 2408 batches | accuracy    0.053 | loss 9.0917430 | time 18.03s \n",
      "| epoch   2 |   400/ 2408 batches | accuracy    0.060 | loss 8.9902926 | time 18.09s \n",
      "| epoch   2 |   600/ 2408 batches | accuracy    0.068 | loss 8.9029674 | time 18.05s \n",
      "| epoch   2 |   800/ 2408 batches | accuracy    0.076 | loss 8.8026907 | time 18.04s \n",
      "| epoch   2 |  1000/ 2408 batches | accuracy    0.083 | loss 8.7087005 | time 18.03s \n",
      "| epoch   2 |  1200/ 2408 batches | accuracy    0.089 | loss 8.6273372 | time 18.02s \n",
      "| epoch   2 |  1400/ 2408 batches | accuracy    0.095 | loss 8.5190931 | time 18.03s \n",
      "| epoch   2 |  1600/ 2408 batches | accuracy    0.104 | loss 8.4348573 | time 18.04s \n",
      "| epoch   2 |  1800/ 2408 batches | accuracy    0.110 | loss 8.3308967 | time 18.02s \n",
      "| epoch   2 |  2000/ 2408 batches | accuracy    0.119 | loss 8.2164899 | time 18.01s \n",
      "| epoch   2 |  2200/ 2408 batches | accuracy    0.127 | loss 8.1514264 | time 18.00s \n",
      "| epoch   2 |  2400/ 2408 batches | accuracy    0.134 | loss 8.0624885 | time 18.03s \n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 219.40s | dev accuracy    0.126 | loss 8.3954061 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |     0/ 2408 batches | accuracy    0.156 | loss 7.2861066 | time  0.11s \n",
      "| epoch   3 |   200/ 2408 batches | accuracy    0.161 | loss 7.2193945 | time 18.05s \n",
      "| epoch   3 |   400/ 2408 batches | accuracy    0.172 | loss 7.1462257 | time 18.01s \n",
      "| epoch   3 |   600/ 2408 batches | accuracy    0.177 | loss 7.1105010 | time 18.01s \n",
      "| epoch   3 |   800/ 2408 batches | accuracy    0.182 | loss 7.0522642 | time 18.02s \n",
      "| epoch   3 |  1000/ 2408 batches | accuracy    0.186 | loss 7.0179206 | time 18.00s \n",
      "| epoch   3 |  1200/ 2408 batches | accuracy    0.190 | loss 6.9445245 | time 17.95s \n",
      "| epoch   3 |  1400/ 2408 batches | accuracy    0.198 | loss 6.8903704 | time 18.02s \n",
      "| epoch   3 |  1600/ 2408 batches | accuracy    0.205 | loss 6.8226209 | time 18.01s \n",
      "| epoch   3 |  1800/ 2408 batches | accuracy    0.209 | loss 6.8026936 | time 18.03s \n",
      "| epoch   3 |  2000/ 2408 batches | accuracy    0.214 | loss 6.7368607 | time 18.03s \n",
      "| epoch   3 |  2200/ 2408 batches | accuracy    0.219 | loss 6.6928060 | time 18.03s \n",
      "| epoch   3 |  2400/ 2408 batches | accuracy    0.226 | loss 6.6102257 | time 17.98s \n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 219.18s | dev accuracy    0.192 | loss 7.5418703 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |     0/ 2408 batches | accuracy    0.254 | loss 6.0571418 | time  0.10s \n",
      "| epoch   4 |   200/ 2408 batches | accuracy    0.270 | loss 5.7253830 | time 18.00s \n",
      "| epoch   4 |   400/ 2408 batches | accuracy    0.267 | loss 5.7472118 | time 18.04s \n",
      "| epoch   4 |   600/ 2408 batches | accuracy    0.269 | loss 5.7511060 | time 18.01s \n",
      "| epoch   4 |   800/ 2408 batches | accuracy    0.273 | loss 5.7186827 | time 18.05s \n",
      "| epoch   4 |  1000/ 2408 batches | accuracy    0.277 | loss 5.6812429 | time 18.04s \n",
      "| epoch   4 |  1200/ 2408 batches | accuracy    0.281 | loss 5.6581259 | time 18.01s \n",
      "| epoch   4 |  1400/ 2408 batches | accuracy    0.284 | loss 5.6594135 | time 18.03s \n",
      "| epoch   4 |  1600/ 2408 batches | accuracy    0.283 | loss 5.6482316 | time 17.99s \n",
      "| epoch   4 |  1800/ 2408 batches | accuracy    0.284 | loss 5.6543680 | time 18.00s \n",
      "| epoch   4 |  2000/ 2408 batches | accuracy    0.293 | loss 5.5830364 | time 18.01s \n",
      "| epoch   4 |  2200/ 2408 batches | accuracy    0.289 | loss 5.5895237 | time 18.05s \n",
      "| epoch   4 |  2400/ 2408 batches | accuracy    0.295 | loss 5.5565117 | time 17.98s \n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 219.20s | dev accuracy    0.239 | loss 7.0153520 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |     0/ 2408 batches | accuracy    0.312 | loss 4.7208924 | time  0.11s \n",
      "| epoch   5 |   200/ 2408 batches | accuracy    0.352 | loss 4.6950448 | time 18.02s \n",
      "| epoch   5 |   400/ 2408 batches | accuracy    0.354 | loss 4.6730296 | time 17.98s \n",
      "| epoch   5 |   600/ 2408 batches | accuracy    0.351 | loss 4.7003128 | time 17.97s \n",
      "| epoch   5 |   800/ 2408 batches | accuracy    0.349 | loss 4.7184040 | time 17.99s \n",
      "| epoch   5 |  1000/ 2408 batches | accuracy    0.349 | loss 4.7415881 | time 18.03s \n",
      "| epoch   5 |  1200/ 2408 batches | accuracy    0.349 | loss 4.7305664 | time 18.03s \n",
      "| epoch   5 |  1400/ 2408 batches | accuracy    0.352 | loss 4.7300609 | time 18.00s \n",
      "| epoch   5 |  1600/ 2408 batches | accuracy    0.348 | loss 4.7421349 | time 18.03s \n",
      "| epoch   5 |  1800/ 2408 batches | accuracy    0.354 | loss 4.7140114 | time 17.97s \n",
      "| epoch   5 |  2000/ 2408 batches | accuracy    0.352 | loss 4.7258829 | time 18.00s \n",
      "| epoch   5 |  2200/ 2408 batches | accuracy    0.352 | loss 4.7380043 | time 17.98s \n",
      "| epoch   5 |  2400/ 2408 batches | accuracy    0.356 | loss 4.7180128 | time 17.97s \n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 219.00s | dev accuracy    0.267 | loss 6.6934665 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |     0/ 2408 batches | accuracy    0.445 | loss 3.7981036 | time  0.11s \n",
      "| epoch   6 |   200/ 2408 batches | accuracy    0.428 | loss 3.8740374 | time 18.01s \n",
      "| epoch   6 |   400/ 2408 batches | accuracy    0.423 | loss 3.8924109 | time 18.02s \n",
      "| epoch   6 |   600/ 2408 batches | accuracy    0.419 | loss 3.9285916 | time 18.04s \n",
      "| epoch   6 |   800/ 2408 batches | accuracy    0.416 | loss 3.9484112 | time 18.00s \n",
      "| epoch   6 |  1000/ 2408 batches | accuracy    0.412 | loss 3.9934798 | time 18.03s \n",
      "| epoch   6 |  1200/ 2408 batches | accuracy    0.414 | loss 3.9831963 | time 17.97s \n",
      "| epoch   6 |  1400/ 2408 batches | accuracy    0.410 | loss 3.9990736 | time 18.01s \n",
      "| epoch   6 |  1600/ 2408 batches | accuracy    0.407 | loss 4.0392346 | time 18.03s \n",
      "| epoch   6 |  1800/ 2408 batches | accuracy    0.408 | loss 4.0297440 | time 18.02s \n",
      "| epoch   6 |  2000/ 2408 batches | accuracy    0.410 | loss 4.0241666 | time 18.00s \n",
      "| epoch   6 |  2200/ 2408 batches | accuracy    0.407 | loss 4.0669587 | time 18.01s \n",
      "| epoch   6 |  2400/ 2408 batches | accuracy    0.409 | loss 4.0486363 | time 18.05s \n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 219.21s | dev accuracy    0.290 | loss 6.4956394 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |     0/ 2408 batches | accuracy    0.531 | loss 3.0859995 | time  0.11s \n",
      "| epoch   7 |   200/ 2408 batches | accuracy    0.503 | loss 3.2282633 | time 18.02s \n",
      "| epoch   7 |   400/ 2408 batches | accuracy    0.489 | loss 3.2943077 | time 18.09s \n",
      "| epoch   7 |   600/ 2408 batches | accuracy    0.486 | loss 3.3171671 | time 18.04s \n",
      "| epoch   7 |   800/ 2408 batches | accuracy    0.479 | loss 3.3420955 | time 18.02s \n",
      "| epoch   7 |  1000/ 2408 batches | accuracy    0.477 | loss 3.3661664 | time 18.01s \n",
      "| epoch   7 |  1200/ 2408 batches | accuracy    0.470 | loss 3.4132737 | time 18.04s \n",
      "| epoch   7 |  1400/ 2408 batches | accuracy    0.466 | loss 3.4303975 | time 18.02s \n",
      "| epoch   7 |  1600/ 2408 batches | accuracy    0.468 | loss 3.4396973 | time 18.03s \n",
      "| epoch   7 |  1800/ 2408 batches | accuracy    0.462 | loss 3.4735188 | time 18.02s \n",
      "| epoch   7 |  2000/ 2408 batches | accuracy    0.461 | loss 3.4834118 | time 18.02s \n",
      "| epoch   7 |  2200/ 2408 batches | accuracy    0.457 | loss 3.5177850 | time 18.03s \n",
      "| epoch   7 |  2400/ 2408 batches | accuracy    0.457 | loss 3.5159950 | time 18.03s \n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time: 219.41s | dev accuracy    0.305 | loss 6.3769480 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |     0/ 2408 batches | accuracy    0.645 | loss 2.4311073 | time  0.11s \n",
      "| epoch   8 |   200/ 2408 batches | accuracy    0.559 | loss 2.7319865 | time 18.00s \n",
      "| epoch   8 |   400/ 2408 batches | accuracy    0.553 | loss 2.7778265 | time 18.03s \n",
      "| epoch   8 |   600/ 2408 batches | accuracy    0.544 | loss 2.8300961 | time 18.00s \n",
      "| epoch   8 |   800/ 2408 batches | accuracy    0.534 | loss 2.8726228 | time 18.04s \n",
      "| epoch   8 |  1000/ 2408 batches | accuracy    0.526 | loss 2.9262966 | time 18.01s \n",
      "| epoch   8 |  1200/ 2408 batches | accuracy    0.528 | loss 2.9318215 | time 18.01s \n",
      "| epoch   8 |  1400/ 2408 batches | accuracy    0.526 | loss 2.9358350 | time 18.02s \n",
      "| epoch   8 |  1600/ 2408 batches | accuracy    0.515 | loss 3.0082383 | time 17.98s \n",
      "| epoch   8 |  1800/ 2408 batches | accuracy    0.515 | loss 3.0089038 | time 18.01s \n",
      "| epoch   8 |  2000/ 2408 batches | accuracy    0.511 | loss 3.0462354 | time 18.01s \n",
      "| epoch   8 |  2200/ 2408 batches | accuracy    0.507 | loss 3.0457240 | time 18.00s \n",
      "| epoch   8 |  2400/ 2408 batches | accuracy    0.507 | loss 3.0599066 | time 18.03s \n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 219.16s | dev accuracy    0.317 | loss 6.3152815 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |     0/ 2408 batches | accuracy    0.570 | loss 2.4996612 | time  0.11s \n",
      "| epoch   9 |   200/ 2408 batches | accuracy    0.608 | loss 2.3581043 | time 18.00s \n",
      "| epoch   9 |   400/ 2408 batches | accuracy    0.602 | loss 2.3916487 | time 17.98s \n",
      "| epoch   9 |   600/ 2408 batches | accuracy    0.590 | loss 2.4516442 | time 18.01s \n",
      "| epoch   9 |   800/ 2408 batches | accuracy    0.587 | loss 2.4682776 | time 17.99s \n",
      "| epoch   9 |  1000/ 2408 batches | accuracy    0.577 | loss 2.5154049 | time 17.98s \n",
      "| epoch   9 |  1200/ 2408 batches | accuracy    0.571 | loss 2.5537979 | time 17.98s \n",
      "| epoch   9 |  1400/ 2408 batches | accuracy    0.563 | loss 2.5990130 | time 18.04s \n",
      "| epoch   9 |  1600/ 2408 batches | accuracy    0.566 | loss 2.6122928 | time 18.03s \n",
      "| epoch   9 |  1800/ 2408 batches | accuracy    0.558 | loss 2.6461592 | time 18.02s \n",
      "| epoch   9 |  2000/ 2408 batches | accuracy    0.558 | loss 2.6521991 | time 17.98s \n",
      "| epoch   9 |  2200/ 2408 batches | accuracy    0.555 | loss 2.6670354 | time 18.04s \n",
      "| epoch   9 |  2400/ 2408 batches | accuracy    0.550 | loss 2.7044236 | time 17.99s \n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 219.08s | dev accuracy    0.325 | loss 6.2960233 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |     0/ 2408 batches | accuracy    0.648 | loss 2.0382774 | time  0.11s \n",
      "| epoch  10 |   200/ 2408 batches | accuracy    0.651 | loss 2.0424288 | time 17.99s \n",
      "| epoch  10 |   400/ 2408 batches | accuracy    0.643 | loss 2.0791933 | time 17.99s \n",
      "| epoch  10 |   600/ 2408 batches | accuracy    0.629 | loss 2.1511013 | time 17.97s \n",
      "| epoch  10 |   800/ 2408 batches | accuracy    0.625 | loss 2.1784502 | time 18.03s \n",
      "| epoch  10 |  1000/ 2408 batches | accuracy    0.616 | loss 2.2232126 | time 18.04s \n",
      "| epoch  10 |  1200/ 2408 batches | accuracy    0.615 | loss 2.2545781 | time 17.99s \n",
      "| epoch  10 |  1400/ 2408 batches | accuracy    0.609 | loss 2.2706927 | time 18.02s \n",
      "| epoch  10 |  1600/ 2408 batches | accuracy    0.603 | loss 2.2994973 | time 18.00s \n",
      "| epoch  10 |  1800/ 2408 batches | accuracy    0.599 | loss 2.3234219 | time 18.02s \n",
      "| epoch  10 |  2000/ 2408 batches | accuracy    0.596 | loss 2.3496935 | time 18.01s \n",
      "| epoch  10 |  2200/ 2408 batches | accuracy    0.592 | loss 2.3853742 | time 18.04s \n",
      "| epoch  10 |  2400/ 2408 batches | accuracy    0.587 | loss 2.4082962 | time 18.00s \n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time: 219.41s | dev accuracy    0.332 | loss 6.2987168 \n",
      "-----------------------------------------------------------\n",
      "| epoch  11 |     0/ 2408 batches | accuracy    0.719 | loss 1.7265798 | time  0.11s \n",
      "| epoch  11 |   200/ 2408 batches | accuracy    0.685 | loss 1.8000619 | time 18.00s \n",
      "| epoch  11 |   400/ 2408 batches | accuracy    0.673 | loss 1.8488363 | time 18.00s \n",
      "| epoch  11 |   600/ 2408 batches | accuracy    0.660 | loss 1.9149765 | time 18.06s \n",
      "| epoch  11 |   800/ 2408 batches | accuracy    0.657 | loss 1.9268063 | time 18.03s \n",
      "| epoch  11 |  1000/ 2408 batches | accuracy    0.648 | loss 1.9712141 | time 18.03s \n",
      "| epoch  11 |  1200/ 2408 batches | accuracy    0.647 | loss 1.9940775 | time 18.00s \n",
      "| epoch  11 |  1400/ 2408 batches | accuracy    0.642 | loss 2.0120492 | time 18.05s \n",
      "| epoch  11 |  1600/ 2408 batches | accuracy    0.634 | loss 2.0712810 | time 18.01s \n",
      "| epoch  11 |  1800/ 2408 batches | accuracy    0.628 | loss 2.0862015 | time 17.99s \n",
      "| epoch  11 |  2000/ 2408 batches | accuracy    0.625 | loss 2.1175508 | time 18.01s \n",
      "| epoch  11 |  2200/ 2408 batches | accuracy    0.625 | loss 2.1309692 | time 18.00s \n",
      "| epoch  11 |  2400/ 2408 batches | accuracy    0.616 | loss 2.1735226 | time 18.02s \n",
      "-----------------------------------------------------------\n",
      "| end of epoch  11 | time: 219.22s | dev accuracy    0.337 | loss 6.3249124 \n",
      "-----------------------------------------------------------\n",
      "| epoch  12 |     0/ 2408 batches | accuracy    0.703 | loss 1.5899179 | time  0.11s \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     previous_model \u001b[38;5;241m=\u001b[39m sizes[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(HYPERPARAMETERS)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mHYPERPARAMETERS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_handler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevious_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 221\u001b[0m, in \u001b[0;36mTrainer.start\u001b[0;34m(self, hyperparameters, data_handler, previous_model)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, hyperparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCHS\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    219\u001b[0m     epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 221\u001b[0m     train_accu_pct, train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m     train_accu_pcts\u001b[38;5;241m.\u001b[39mappend(train_accu_pct)\n\u001b[1;32m    223\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[1], line 140\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, dataloader, epoch)\u001b[0m\n\u001b[1;32m    137\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m    138\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m--> 140\u001b[0m batch_acc \u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mpredicted_answer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m answer\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    143\u001b[0m running_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_size\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer()\n",
    "\n",
    "embed_hidden_sizes = [(50,100)] \n",
    "\n",
    "for i in range(len(embed_hidden_sizes)):\n",
    "    sizes = embed_hidden_sizes[i]\n",
    "    HYPERPARAMETERS = {\n",
    "        'EPOCHS': 25,\n",
    "        'LR': 0.001,\n",
    "        'BATCH_SIZE': 256,\n",
    "        'EMBEDDING_LAYER_SIZE': sizes[0],\n",
    "        'HIDDEN_SIZE': sizes[1]\n",
    "    }\n",
    "    previous_model = None\n",
    "    if len(sizes) > 2:\n",
    "        previous_model = sizes[2]\n",
    "    \n",
    "    print(HYPERPARAMETERS)\n",
    "    trainer.start(HYPERPARAMETERS, data_handler, previous_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e42449-f504-4849-8b41-0fd98137dff3",
   "metadata": {},
   "source": [
    "## Sample from the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78c87adc-1a5f-4f19-a24c-811babf70d54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capital of canada: hanoi\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "def predict(clue):\n",
    "    with torch.no_grad():\n",
    "        # create clue tensor and pad\n",
    "        clue_indicies = data_handler.clue_pipeline(clue)\n",
    "        clue_indicies += [PADDING_TOKEN_INDEX] * (PAD_TO_SIZE - len(clue_indicies))\n",
    "        output = trained_model(torch.tensor(clue_indicies))\n",
    "        return output.argmax(1).item()\n",
    "    \n",
    "def load_model(training_dir):\n",
    "    with open(os.path.join(training_dir, 'training-results.json'), 'r') as f:\n",
    "        result = json.load(f)\n",
    "        last_epoch = len(result['dev_losses'])\n",
    "        embed_size = result['hyperparameters']['EMBEDDING_LAYER_SIZE']\n",
    "        hidden_size = result['hyperparameters']['HIDDEN_SIZE']\n",
    "\n",
    "        trained_model = CrosswordModel(len(clues_vocab), embed_size, PAD_TO_SIZE, hidden_size, len(answers_vocab), device)\n",
    "        trained_model.load_state_dict(torch.load(os.path.join(training_dir, f'model-epoch-{last_epoch}.pt')))\n",
    "        \n",
    "        return trained_model\n",
    "\n",
    "# load model from saved file\n",
    "training_dir = 'training_results/20230321-100932-training'\n",
    "trained_model = load_model(training_dir)\n",
    "trained_model = trained_model.to(\"cpu\")\n",
    "trained_model.eval()\n",
    "\n",
    "test_clue = 'capital of canada'\n",
    "print(f'{test_clue}: {data_handler.answers_vocab.get_itos()[predict(test_clue)]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
