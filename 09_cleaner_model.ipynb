{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f7c2d9e-9c5d-4062-9616-6304d85b91ad",
   "metadata": {
    "tags": []
   },
   "source": [
    "# PyTorch, a cleaner story\n",
    "\n",
    "In [07_pytorch_exploration.ipynb](07_pytorch_exploration.ipynb) we built our first network _actually_ using PyTorch, but it was pretty messy and hard to follow. Let's continue that work and see what else we can learn/optimize (both the human and the model ðŸ™ƒ)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5d121da-9dbf-4b44-8720-fd519102d742",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import pandas as pd\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "PADDING_TOKEN_INDEX = 0\n",
    "PAD_TO_SIZE = 45\n",
    "\n",
    "class CrosswordClueAnswersDataset(Dataset):\n",
    "    \"\"\"Crossword clues and answers dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, csv_file):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with clues and answers.\n",
    "        \"\"\"\n",
    "        self.values = pd.read_csv(csv_file, keep_default_na=False).values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.values)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        data = self.values[idx, :]\n",
    "        return (data[0], data[1])\n",
    "\n",
    "class CrosswordModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, padding_size, hidden_size, num_class, device):\n",
    "        super(CrosswordModel, self).__init__()\n",
    "        self.C = torch.nn.Embedding(vocab_size, embed_dim, device=device)\n",
    "        self.W1 = torch.nn.Linear(embed_dim * padding_size, hidden_size, device=device)\n",
    "        self.M = nn.Tanh()\n",
    "        self.W2 = torch.nn.Linear(hidden_size, num_class, device=device)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.C.weight.data.uniform_(-initrange, initrange)\n",
    "        self.W1.weight.data.uniform_(-initrange, initrange)\n",
    "        self.W1.bias.data.zero_()\n",
    "        self.W2.weight.data.uniform_(-initrange, initrange)\n",
    "        self.W2.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text):\n",
    "        emb = self.C(text)\n",
    "        h = self.M(self.W1(emb.view(-1, self.W1.in_features)))\n",
    "        return self.W2(h)\n",
    "    \n",
    "class DataHandler():\n",
    "    \n",
    "    def __init__(self, device):\n",
    "        self.tokenizer = get_tokenizer('basic_english')\n",
    "        self.device = device\n",
    "\n",
    "    def yield_clues(self, data_iter):\n",
    "        for _, clue in data_iter:\n",
    "            yield self.tokenizer(clue)\n",
    "\n",
    "    def yield_answers(self, data_iter):\n",
    "        for answer, _ in data_iter:\n",
    "            yield self.tokenizer(answer)\n",
    "            \n",
    "    def clue_pipeline(self, x):\n",
    "        return self.clues_vocab(self.tokenizer(x))\n",
    "    \n",
    "    def answer_pipeline(self, x):\n",
    "        return self.answers_vocab([x])[0]\n",
    "        \n",
    "    def collate_batch(self, batch):\n",
    "        answer_list, clue_list = [], []\n",
    "\n",
    "        for (answer, clue) in batch:\n",
    "            clue_indicies = self.clue_pipeline(clue)\n",
    "            clue_indicies += [PADDING_TOKEN_INDEX] * (PAD_TO_SIZE - len(clue_indicies))\n",
    "            clue_list.append(clue_indicies)\n",
    "\n",
    "            answer_list.append(self.answer_pipeline(answer))\n",
    "\n",
    "        answer_list = torch.tensor(answer_list).to(device)\n",
    "        clue_list = torch.tensor(clue_list).to(device)\n",
    "\n",
    "        return answer_list, clue_list\n",
    "\n",
    "    def createDatasets(self):\n",
    "        \"\"\"\n",
    "        Creates train, test, and dev datasets.\n",
    "        Returns a tuple of (test, train, dev, all)\n",
    "        \"\"\"\n",
    "        self.dataset = CrosswordClueAnswersDataset(\"cleaned_data/clean_2.csv\")\n",
    "        train_size = int(0.8 * len(self.dataset))\n",
    "        dev_size = int(0.1 * len(self.dataset))\n",
    "        test_size = len(self.dataset) - train_size - dev_size\n",
    "        # this manual_seed is important to ensure that we consistently split the dataset\n",
    "        g = torch.Generator().manual_seed(42)\n",
    "        self.train_dataset, self.test_dataset, self.dev_dataset = torch.utils.data.random_split(self.dataset, [train_size, test_size, dev_size], generator=g)\n",
    "\n",
    "        return (self.train_dataset, self.test_dataset, self.dev_dataset, self.dataset)\n",
    "\n",
    "    def createVocabs(self):\n",
    "        \"\"\"\n",
    "        Creates answers and clues vocabularies\n",
    "        Returns tuple of (answers_vocab, clues_vocab)\n",
    "        \"\"\"\n",
    "        self.answers_vocab = build_vocab_from_iterator(self.yield_answers(train_dataset), specials=['<unk>'])\n",
    "        # add this in case we see an answer in the test/dev set that we don't have in the training set\n",
    "        self.answers_vocab.set_default_index(0)\n",
    "        self.clues_vocab = build_vocab_from_iterator(self.yield_clues(train_dataset), specials=['<pad>', '<unk>'])\n",
    "        self.clues_vocab.set_default_index(1)\n",
    "        return (self.answers_vocab, self.clues_vocab)\n",
    "\n",
    "def train(model, optimizer, criterion, dataloader, epoch):\n",
    "    model.train()\n",
    "    \n",
    "    # stats for each log interval\n",
    "    log_interval_batch_count, log_interval_count, log_interval_loss, log_interval_acc,  = 0, 0, 0, 0\n",
    "    log_interval = 200\n",
    "    log_interval_start_time = time.time()\n",
    "    \n",
    "    # stats for the entire dataset\n",
    "    running_loss, running_acc, running_count = 0, 0, 0\n",
    "\n",
    "    for idx, (answer, clue) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predicted_answer = model(clue)\n",
    "        loss = criterion(predicted_answer, answer)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_acc = (predicted_answer.argmax(1) == answer).sum().item()\n",
    "        batch_size = answer.size(0)\n",
    "        \n",
    "        running_count += batch_size\n",
    "        running_loss += loss.item()\n",
    "        running_acc += batch_acc\n",
    "\n",
    "        log_interval_batch_count += 1\n",
    "        log_interval_count += batch_size\n",
    "        log_interval_loss += loss.item() \n",
    "        log_interval_acc += batch_acc\n",
    "\n",
    "        \n",
    "        if idx % log_interval == 0:\n",
    "            elapsed = time.time() - log_interval_start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f} | loss {:8.7f} | time {:5.2f}s '.format(epoch, idx, len(dataloader),\n",
    "                                              log_interval_acc / log_interval_count, log_interval_loss / log_interval_batch_count, elapsed))\n",
    "            log_interval_batch_count, log_interval_count, log_interval_acc, log_interval_loss = 0, 0, 0, 0\n",
    "            log_interval_start_time = time.time()\n",
    "    \n",
    "    return running_acc / running_count, running_loss / len(dataloader)\n",
    "\n",
    "class Trainer:\n",
    "    def evaluate(self, model, criterion, dataloader):\n",
    "        \"\"\"\n",
    "        Evaluate the model against a dataset\n",
    "        Returns a tuple of (acurate_pct, loss)\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        total_acc, total_count, running_loss = 0, 0, 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx, (answer, clue) in enumerate(dataloader):\n",
    "                predicted_answer = model(clue)\n",
    "                loss = criterion(predicted_answer, answer).item()\n",
    "                total_acc += (predicted_answer.argmax(1) == answer).sum().item()\n",
    "                total_count += answer.size(0)\n",
    "                running_loss += loss\n",
    "        return total_acc / total_count, running_loss / len(dataloader)\n",
    "\n",
    "    def start(self, hyperparameters, data_handler, previous_model):\n",
    "        \"\"\"\n",
    "        Creates a model and trains it based on the hyperparameters\n",
    "        Model output is saved\n",
    "        Returns accu_pct, loss\n",
    "        \"\"\"\n",
    "        # shuffle the training dataloader so we go through different batches each time\n",
    "        train_dataloader = DataLoader(data_handler.train_dataset, batch_size=hyperparameters['BATCH_SIZE'], shuffle=True, collate_fn=data_handler.collate_batch)\n",
    "        dev_dataloader = DataLoader(data_handler.dev_dataset, batch_size=hyperparameters['BATCH_SIZE'], shuffle=False, collate_fn=data_handler.collate_batch)\n",
    "\n",
    "        num_class = len(data_handler.answers_vocab)\n",
    "        vocab_size = len(data_handler.clues_vocab)\n",
    "        model = CrosswordModel(vocab_size, hyperparameters['EMBEDDING_LAYER_SIZE'], PAD_TO_SIZE, hyperparameters['HIDDEN_SIZE'], num_class, device)\n",
    "\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=hyperparameters['LR'])\n",
    "        \n",
    "        # load previous model if we have one\n",
    "        if previous_model is not None:\n",
    "            #TODO: would be nice to just continue this run - we'd need to save state for the optimizer, load up the training dict, and maybe some other stuff\n",
    "            print('Loading from previous model:', previous_model)\n",
    "            model.load_state_dict(torch.load(previous_model))\n",
    "            dev_accu_pct, dev_loss = self.evaluate(model, criterion, dev_dataloader)\n",
    "            print(f'Starting with model: {dev_accu_pct} accuracy; {dev_loss} loss;')\n",
    "        \n",
    "        # setup directories and files for output\n",
    "        timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "        train_dir = os.path.join('training_results', timestr + '-training')\n",
    "        if not os.path.exists(train_dir):\n",
    "            os.makedirs(train_dir)\n",
    "        results_filename = os.path.join(train_dir, 'training-results.json')\n",
    "        \n",
    "        # stats to track for eeach epoch\n",
    "        dev_accu_pcts, dev_losses = [], []\n",
    "        train_accu_pcts, train_losses = [], []\n",
    "        lrs, elapsed_times = [], []\n",
    "\n",
    "        for epoch in range(1, hyperparameters['EPOCHS'] + 1):\n",
    "            epoch_start_time = time.time()\n",
    "            \n",
    "            train_accu_pct, train_loss = train(model, optimizer, criterion, train_dataloader, epoch)\n",
    "            train_accu_pcts.append(train_accu_pct)\n",
    "            train_losses.append(train_loss)\n",
    "            \n",
    "            dev_accu_pct, dev_loss = self.evaluate(model, criterion, dev_dataloader)\n",
    "            dev_accu_pcts.append(dev_accu_pct)\n",
    "            dev_losses.append(dev_loss)\n",
    "            # learning rate doesn't change for now. Maybe we should log some detail from the optimzer?\n",
    "            lrs.append(hyperparameters['LR'])\n",
    "            \n",
    "            epoch_elapsed_time = time.time() - epoch_start_time\n",
    "            elapsed_times.append(epoch_elapsed_time)\n",
    "            \n",
    "            print('-' * 59)\n",
    "            print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "                  'dev accuracy {:8.3f} | loss {:8.7f} '.format(epoch,\n",
    "                                                   epoch_elapsed_time,\n",
    "                                                   dev_accu_pct, dev_loss))\n",
    "            print('-' * 59)\n",
    "            \n",
    "            \n",
    "            # save results and model for this epoch\n",
    "            model_filename = os.path.join(train_dir, 'model-epoch-' + str(epoch) + '.pt')\n",
    "            training_dict = {\n",
    "                'hyperparameters': hyperparameters,\n",
    "                'dev_accu_pcts': dev_accu_pcts,\n",
    "                'dev_losses': dev_losses,\n",
    "                'train_accu_pcts': train_accu_pcts,\n",
    "                'train_losses': train_losses,\n",
    "                'elapsed_times': elapsed_times,\n",
    "                'learning_rates': lrs,\n",
    "                'model': model_filename,\n",
    "                'previous_model': previous_model\n",
    "            }\n",
    "\n",
    "            with open(results_filename, 'w') as file:\n",
    "                 file.write(json.dumps(training_dict))\n",
    "            torch.save(model.state_dict(), model_filename)\n",
    "\n",
    "        return model, dev_accu_pcts, dev_losses, train_accu_pcts, train_losses, elapsed_times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e689731e-b5eb-48a4-9606-ef96c6d6051b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on: mps\n",
      "len(dataset)=770361\n",
      "len(train_dataset)=616288\n",
      "len(test_dataset)=77037\n",
      "len(dev_dataset)=77036\n",
      "\n",
      "len(answers_vocab)=60323,len(clues_vocab)=77891\n",
      "\n",
      "CPU times: user 4.24 s, sys: 69.3 ms, total: 4.31 s\n",
      "Wall time: 4.32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# attempt to run on mps - will do work on the GPU for MacOS\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "#device = 'cpu'\n",
    "print(f'Running on:', device)\n",
    "\n",
    "data_handler = DataHandler(device)\n",
    "\n",
    "# load dataset, split data\n",
    "train_dataset, test_dataset, dev_dataset, dataset = data_handler.createDatasets()\n",
    "print(f'{len(dataset)=}\\n{len(train_dataset)=}\\n{len(test_dataset)=}\\n{len(dev_dataset)=}\\n')\n",
    "\n",
    "# build vocabulary\n",
    "answers_vocab, clues_vocab = data_handler.createVocabs()\n",
    "\n",
    "print(f'{len(answers_vocab)=},{len(clues_vocab)=}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8f0ef2e-4a31-44ab-9580-b5ed5447fa1b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'EPOCHS': 25, 'LR': 0.001, 'BATCH_SIZE': 256, 'EMBEDDING_LAYER_SIZE': 8, 'HIDDEN_SIZE': 50}\n",
      "| epoch   1 |     0/ 2408 batches | accuracy    0.000 | loss 12.4251575 | time  0.17s \n",
      "| epoch   1 |   200/ 2408 batches | accuracy    0.000 | loss 11.1338400 | time 13.13s \n",
      "| epoch   1 |   400/ 2408 batches | accuracy    0.000 | loss 10.6304643 | time 13.12s \n",
      "| epoch   1 |   600/ 2408 batches | accuracy    0.001 | loss 10.4801537 | time 13.10s \n",
      "| epoch   1 |   800/ 2408 batches | accuracy    0.001 | loss 10.3871909 | time 13.09s \n",
      "| epoch   1 |  1000/ 2408 batches | accuracy    0.001 | loss 10.3349444 | time 13.11s \n",
      "| epoch   1 |  1200/ 2408 batches | accuracy    0.002 | loss 10.2680255 | time 13.11s \n",
      "| epoch   1 |  1400/ 2408 batches | accuracy    0.002 | loss 10.2331191 | time 13.11s \n",
      "| epoch   1 |  1600/ 2408 batches | accuracy    0.002 | loss 10.1647533 | time 13.08s \n",
      "| epoch   1 |  1800/ 2408 batches | accuracy    0.003 | loss 10.1315804 | time 13.16s \n",
      "| epoch   1 |  2000/ 2408 batches | accuracy    0.003 | loss 10.0940204 | time 13.12s \n",
      "| epoch   1 |  2200/ 2408 batches | accuracy    0.005 | loss 10.0367047 | time 13.14s \n",
      "| epoch   1 |  2400/ 2408 batches | accuracy    0.005 | loss 9.9662364 | time 13.07s \n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 160.12s | dev accuracy    0.006 | loss 9.9593986 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |     0/ 2408 batches | accuracy    0.016 | loss 9.6516180 | time  0.08s \n",
      "| epoch   2 |   200/ 2408 batches | accuracy    0.007 | loss 9.6806736 | time 13.12s \n",
      "| epoch   2 |   400/ 2408 batches | accuracy    0.009 | loss 9.6021101 | time 13.14s \n",
      "| epoch   2 |   600/ 2408 batches | accuracy    0.011 | loss 9.5286407 | time 13.13s \n",
      "| epoch   2 |   800/ 2408 batches | accuracy    0.013 | loss 9.4733370 | time 13.16s \n",
      "| epoch   2 |  1000/ 2408 batches | accuracy    0.015 | loss 9.4385899 | time 13.15s \n",
      "| epoch   2 |  1200/ 2408 batches | accuracy    0.017 | loss 9.3663628 | time 13.09s \n",
      "| epoch   2 |  1400/ 2408 batches | accuracy    0.020 | loss 9.3138706 | time 13.12s \n",
      "| epoch   2 |  1600/ 2408 batches | accuracy    0.022 | loss 9.2661521 | time 13.10s \n",
      "| epoch   2 |  1800/ 2408 batches | accuracy    0.024 | loss 9.2215262 | time 13.11s \n",
      "| epoch   2 |  2000/ 2408 batches | accuracy    0.025 | loss 9.1670865 | time 13.11s \n",
      "| epoch   2 |  2200/ 2408 batches | accuracy    0.029 | loss 9.1196217 | time 13.14s \n",
      "| epoch   2 |  2400/ 2408 batches | accuracy    0.030 | loss 9.0794096 | time 13.14s \n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 160.15s | dev accuracy    0.029 | loss 9.3084741 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |     0/ 2408 batches | accuracy    0.047 | loss 8.6842842 | time  0.08s \n",
      "| epoch   3 |   200/ 2408 batches | accuracy    0.035 | loss 8.7398671 | time 13.15s \n",
      "| epoch   3 |   400/ 2408 batches | accuracy    0.038 | loss 8.7112160 | time 13.12s \n",
      "| epoch   3 |   600/ 2408 batches | accuracy    0.040 | loss 8.6779438 | time 13.14s \n",
      "| epoch   3 |   800/ 2408 batches | accuracy    0.043 | loss 8.6466475 | time 13.12s \n",
      "| epoch   3 |  1000/ 2408 batches | accuracy    0.045 | loss 8.6195462 | time 13.14s \n",
      "| epoch   3 |  1200/ 2408 batches | accuracy    0.048 | loss 8.5711458 | time 13.15s \n",
      "| epoch   3 |  1400/ 2408 batches | accuracy    0.048 | loss 8.5561305 | time 13.15s \n",
      "| epoch   3 |  1600/ 2408 batches | accuracy    0.051 | loss 8.5380505 | time 13.13s \n",
      "| epoch   3 |  1800/ 2408 batches | accuracy    0.054 | loss 8.5092989 | time 13.15s \n",
      "| epoch   3 |  2000/ 2408 batches | accuracy    0.054 | loss 8.4671366 | time 13.15s \n",
      "| epoch   3 |  2200/ 2408 batches | accuracy    0.056 | loss 8.4623954 | time 13.14s \n",
      "| epoch   3 |  2400/ 2408 batches | accuracy    0.057 | loss 8.4481548 | time 13.13s \n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 160.32s | dev accuracy    0.056 | loss 8.9119296 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |     0/ 2408 batches | accuracy    0.070 | loss 7.9157543 | time  0.08s \n",
      "| epoch   4 |   200/ 2408 batches | accuracy    0.066 | loss 8.0764334 | time 13.14s \n",
      "| epoch   4 |   400/ 2408 batches | accuracy    0.067 | loss 8.0496042 | time 13.12s \n",
      "| epoch   4 |   600/ 2408 batches | accuracy    0.067 | loss 8.0656974 | time 13.13s \n",
      "| epoch   4 |   800/ 2408 batches | accuracy    0.071 | loss 8.0443091 | time 13.12s \n",
      "| epoch   4 |  1000/ 2408 batches | accuracy    0.074 | loss 8.0104962 | time 13.12s \n",
      "| epoch   4 |  1200/ 2408 batches | accuracy    0.073 | loss 8.0254473 | time 13.13s \n",
      "| epoch   4 |  1400/ 2408 batches | accuracy    0.076 | loss 8.0033698 | time 13.12s \n",
      "| epoch   4 |  1600/ 2408 batches | accuracy    0.080 | loss 7.9656979 | time 13.24s \n",
      "| epoch   4 |  1800/ 2408 batches | accuracy    0.080 | loss 7.9528763 | time 13.18s \n",
      "| epoch   4 |  2000/ 2408 batches | accuracy    0.082 | loss 7.9574201 | time 13.10s \n",
      "| epoch   4 |  2200/ 2408 batches | accuracy    0.084 | loss 7.9141888 | time 13.14s \n",
      "| epoch   4 |  2400/ 2408 batches | accuracy    0.085 | loss 7.9166448 | time 13.10s \n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 160.27s | dev accuracy    0.077 | loss 8.6363649 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |     0/ 2408 batches | accuracy    0.105 | loss 7.4174447 | time  0.07s \n",
      "| epoch   5 |   200/ 2408 batches | accuracy    0.094 | loss 7.5621920 | time 13.10s \n",
      "| epoch   5 |   400/ 2408 batches | accuracy    0.096 | loss 7.5696473 | time 13.12s \n",
      "| epoch   5 |   600/ 2408 batches | accuracy    0.093 | loss 7.6012734 | time 13.10s \n",
      "| epoch   5 |   800/ 2408 batches | accuracy    0.096 | loss 7.5586654 | time 13.13s \n",
      "| epoch   5 |  1000/ 2408 batches | accuracy    0.098 | loss 7.5362559 | time 13.17s \n",
      "| epoch   5 |  1200/ 2408 batches | accuracy    0.098 | loss 7.5562038 | time 13.58s \n",
      "| epoch   5 |  1400/ 2408 batches | accuracy    0.103 | loss 7.5432522 | time 13.23s \n",
      "| epoch   5 |  1600/ 2408 batches | accuracy    0.103 | loss 7.5452703 | time 13.12s \n",
      "| epoch   5 |  1800/ 2408 batches | accuracy    0.105 | loss 7.5039980 | time 13.09s \n",
      "| epoch   5 |  2000/ 2408 batches | accuracy    0.107 | loss 7.5163581 | time 13.13s \n",
      "| epoch   5 |  2200/ 2408 batches | accuracy    0.108 | loss 7.5161742 | time 13.20s \n",
      "| epoch   5 |  2400/ 2408 batches | accuracy    0.108 | loss 7.5152038 | time 13.13s \n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 160.82s | dev accuracy    0.095 | loss 8.4340107 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |     0/ 2408 batches | accuracy    0.148 | loss 6.8920975 | time  0.08s \n",
      "| epoch   6 |   200/ 2408 batches | accuracy    0.120 | loss 7.1252979 | time 13.13s \n",
      "| epoch   6 |   400/ 2408 batches | accuracy    0.119 | loss 7.1573961 | time 13.13s \n",
      "| epoch   6 |   600/ 2408 batches | accuracy    0.119 | loss 7.1827326 | time 13.10s \n",
      "| epoch   6 |   800/ 2408 batches | accuracy    0.121 | loss 7.2014272 | time 13.12s \n",
      "| epoch   6 |  1000/ 2408 batches | accuracy    0.122 | loss 7.1814976 | time 13.14s \n",
      "| epoch   6 |  1200/ 2408 batches | accuracy    0.123 | loss 7.1883990 | time 13.07s \n",
      "| epoch   6 |  1400/ 2408 batches | accuracy    0.126 | loss 7.1632307 | time 13.08s \n",
      "| epoch   6 |  1600/ 2408 batches | accuracy    0.123 | loss 7.1781086 | time 13.09s \n",
      "| epoch   6 |  1800/ 2408 batches | accuracy    0.126 | loss 7.1571992 | time 13.10s \n",
      "| epoch   6 |  2000/ 2408 batches | accuracy    0.127 | loss 7.1612581 | time 13.11s \n",
      "| epoch   6 |  2200/ 2408 batches | accuracy    0.128 | loss 7.1690691 | time 13.10s \n",
      "| epoch   6 |  2400/ 2408 batches | accuracy    0.128 | loss 7.1585052 | time 13.12s \n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 159.89s | dev accuracy    0.112 | loss 8.2824412 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |     0/ 2408 batches | accuracy    0.129 | loss 7.1014590 | time  0.08s \n",
      "| epoch   7 |   200/ 2408 batches | accuracy    0.142 | loss 6.7780599 | time 13.14s \n",
      "| epoch   7 |   400/ 2408 batches | accuracy    0.139 | loss 6.8368318 | time 13.12s \n",
      "| epoch   7 |   600/ 2408 batches | accuracy    0.138 | loss 6.8465785 | time 13.14s \n",
      "| epoch   7 |   800/ 2408 batches | accuracy    0.143 | loss 6.8368372 | time 13.14s \n",
      "| epoch   7 |  1000/ 2408 batches | accuracy    0.145 | loss 6.8290912 | time 13.16s \n",
      "| epoch   7 |  1200/ 2408 batches | accuracy    0.144 | loss 6.8597664 | time 13.14s \n",
      "| epoch   7 |  1400/ 2408 batches | accuracy    0.143 | loss 6.8796507 | time 13.16s \n",
      "| epoch   7 |  1600/ 2408 batches | accuracy    0.145 | loss 6.8631733 | time 13.15s \n",
      "| epoch   7 |  1800/ 2408 batches | accuracy    0.147 | loss 6.8619867 | time 13.10s \n",
      "| epoch   7 |  2000/ 2408 batches | accuracy    0.146 | loss 6.8621476 | time 13.15s \n",
      "| epoch   7 |  2200/ 2408 batches | accuracy    0.146 | loss 6.8569273 | time 13.14s \n",
      "| epoch   7 |  2400/ 2408 batches | accuracy    0.146 | loss 6.8943180 | time 13.14s \n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time: 160.37s | dev accuracy    0.125 | loss 8.1775642 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |     0/ 2408 batches | accuracy    0.188 | loss 6.5084105 | time  0.08s \n",
      "| epoch   8 |   200/ 2408 batches | accuracy    0.161 | loss 6.5158110 | time 13.13s \n",
      "| epoch   8 |   400/ 2408 batches | accuracy    0.160 | loss 6.5287779 | time 13.15s \n",
      "| epoch   8 |   600/ 2408 batches | accuracy    0.160 | loss 6.5426390 | time 13.11s \n",
      "| epoch   8 |   800/ 2408 batches | accuracy    0.160 | loss 6.5434920 | time 13.09s \n",
      "| epoch   8 |  1000/ 2408 batches | accuracy    0.159 | loss 6.5817076 | time 13.13s \n",
      "| epoch   8 |  1200/ 2408 batches | accuracy    0.165 | loss 6.5592658 | time 13.15s \n",
      "| epoch   8 |  1400/ 2408 batches | accuracy    0.162 | loss 6.5795421 | time 13.09s \n",
      "| epoch   8 |  1600/ 2408 batches | accuracy    0.166 | loss 6.5742805 | time 13.11s \n",
      "| epoch   8 |  1800/ 2408 batches | accuracy    0.165 | loss 6.6003551 | time 13.12s \n",
      "| epoch   8 |  2000/ 2408 batches | accuracy    0.159 | loss 6.6231405 | time 13.11s \n",
      "| epoch   8 |  2200/ 2408 batches | accuracy    0.163 | loss 6.6333919 | time 13.12s \n",
      "| epoch   8 |  2400/ 2408 batches | accuracy    0.164 | loss 6.6125626 | time 13.07s \n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 160.03s | dev accuracy    0.137 | loss 8.1030802 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |     0/ 2408 batches | accuracy    0.172 | loss 6.3283587 | time  0.08s \n",
      "| epoch   9 |   200/ 2408 batches | accuracy    0.181 | loss 6.2233487 | time 13.12s \n",
      "| epoch   9 |   400/ 2408 batches | accuracy    0.179 | loss 6.2545138 | time 13.11s \n",
      "| epoch   9 |   600/ 2408 batches | accuracy    0.179 | loss 6.2822309 | time 13.09s \n",
      "| epoch   9 |   800/ 2408 batches | accuracy    0.178 | loss 6.2964999 | time 13.08s \n",
      "| epoch   9 |  1000/ 2408 batches | accuracy    0.176 | loss 6.3419707 | time 13.05s \n",
      "| epoch   9 |  1200/ 2408 batches | accuracy    0.177 | loss 6.3435105 | time 13.11s \n",
      "| epoch   9 |  1400/ 2408 batches | accuracy    0.179 | loss 6.3461344 | time 13.08s \n",
      "| epoch   9 |  1600/ 2408 batches | accuracy    0.179 | loss 6.3333548 | time 13.10s \n",
      "| epoch   9 |  1800/ 2408 batches | accuracy    0.178 | loss 6.3627430 | time 13.15s \n",
      "| epoch   9 |  2000/ 2408 batches | accuracy    0.177 | loss 6.3704624 | time 13.14s \n",
      "| epoch   9 |  2200/ 2408 batches | accuracy    0.176 | loss 6.4150571 | time 13.11s \n",
      "| epoch   9 |  2400/ 2408 batches | accuracy    0.178 | loss 6.4305230 | time 13.10s \n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 159.86s | dev accuracy    0.146 | loss 8.0290886 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |     0/ 2408 batches | accuracy    0.203 | loss 5.9084425 | time  0.08s \n",
      "| epoch  10 |   200/ 2408 batches | accuracy    0.198 | loss 6.0107099 | time 13.10s \n",
      "| epoch  10 |   400/ 2408 batches | accuracy    0.194 | loss 6.0383405 | time 13.11s \n",
      "| epoch  10 |   600/ 2408 batches | accuracy    0.195 | loss 6.0480413 | time 13.11s \n",
      "| epoch  10 |   800/ 2408 batches | accuracy    0.194 | loss 6.0935228 | time 13.11s \n",
      "| epoch  10 |  1000/ 2408 batches | accuracy    0.194 | loss 6.0980045 | time 13.09s \n",
      "| epoch  10 |  1200/ 2408 batches | accuracy    0.190 | loss 6.1417402 | time 13.12s \n",
      "| epoch  10 |  1400/ 2408 batches | accuracy    0.192 | loss 6.1433290 | time 13.10s \n",
      "| epoch  10 |  1600/ 2408 batches | accuracy    0.195 | loss 6.1457135 | time 13.10s \n",
      "| epoch  10 |  1800/ 2408 batches | accuracy    0.194 | loss 6.1631716 | time 13.08s \n",
      "| epoch  10 |  2000/ 2408 batches | accuracy    0.194 | loss 6.1556230 | time 13.14s \n",
      "| epoch  10 |  2200/ 2408 batches | accuracy    0.193 | loss 6.1951899 | time 13.09s \n",
      "| epoch  10 |  2400/ 2408 batches | accuracy    0.193 | loss 6.1925392 | time 13.08s \n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time: 159.88s | dev accuracy    0.154 | loss 7.9814670 \n",
      "-----------------------------------------------------------\n",
      "| epoch  11 |     0/ 2408 batches | accuracy    0.289 | loss 5.3414450 | time  0.08s \n",
      "| epoch  11 |   200/ 2408 batches | accuracy    0.213 | loss 5.8130401 | time 13.13s \n",
      "| epoch  11 |   400/ 2408 batches | accuracy    0.207 | loss 5.8604376 | time 13.13s \n",
      "| epoch  11 |   600/ 2408 batches | accuracy    0.212 | loss 5.8570549 | time 13.12s \n",
      "| epoch  11 |   800/ 2408 batches | accuracy    0.208 | loss 5.8782927 | time 13.09s \n",
      "| epoch  11 |  1000/ 2408 batches | accuracy    0.208 | loss 5.9117937 | time 13.08s \n",
      "| epoch  11 |  1200/ 2408 batches | accuracy    0.209 | loss 5.9100559 | time 13.13s \n",
      "| epoch  11 |  1400/ 2408 batches | accuracy    0.207 | loss 5.9336359 | time 13.14s \n",
      "| epoch  11 |  1600/ 2408 batches | accuracy    0.208 | loss 5.9611433 | time 13.08s \n",
      "| epoch  11 |  1800/ 2408 batches | accuracy    0.208 | loss 5.9728477 | time 13.08s \n",
      "| epoch  11 |  2000/ 2408 batches | accuracy    0.207 | loss 5.9887196 | time 13.11s \n",
      "| epoch  11 |  2200/ 2408 batches | accuracy    0.208 | loss 6.0051178 | time 13.10s \n",
      "| epoch  11 |  2400/ 2408 batches | accuracy    0.202 | loss 6.0373890 | time 13.10s \n",
      "-----------------------------------------------------------\n",
      "| end of epoch  11 | time: 159.93s | dev accuracy    0.162 | loss 7.9450024 \n",
      "-----------------------------------------------------------\n",
      "| epoch  12 |     0/ 2408 batches | accuracy    0.246 | loss 5.5962820 | time  0.07s \n",
      "| epoch  12 |   200/ 2408 batches | accuracy    0.229 | loss 5.6406731 | time 13.12s \n",
      "| epoch  12 |   400/ 2408 batches | accuracy    0.230 | loss 5.6313316 | time 13.15s \n",
      "| epoch  12 |   600/ 2408 batches | accuracy    0.222 | loss 5.6938119 | time 13.12s \n",
      "| epoch  12 |   800/ 2408 batches | accuracy    0.221 | loss 5.7024551 | time 13.16s \n",
      "| epoch  12 |  1000/ 2408 batches | accuracy    0.222 | loss 5.7325354 | time 13.13s \n",
      "| epoch  12 |  1200/ 2408 batches | accuracy    0.220 | loss 5.7731963 | time 13.12s \n",
      "| epoch  12 |  1400/ 2408 batches | accuracy    0.220 | loss 5.7782749 | time 13.12s \n",
      "| epoch  12 |  1600/ 2408 batches | accuracy    0.217 | loss 5.8143746 | time 13.12s \n",
      "| epoch  12 |  1800/ 2408 batches | accuracy    0.220 | loss 5.8028447 | time 13.09s \n",
      "| epoch  12 |  2000/ 2408 batches | accuracy    0.218 | loss 5.8099411 | time 13.12s \n",
      "| epoch  12 |  2200/ 2408 batches | accuracy    0.218 | loss 5.8383421 | time 13.10s \n",
      "| epoch  12 |  2400/ 2408 batches | accuracy    0.220 | loss 5.8399290 | time 13.10s \n",
      "-----------------------------------------------------------\n",
      "| end of epoch  12 | time: 160.11s | dev accuracy    0.168 | loss 7.9225671 \n",
      "-----------------------------------------------------------\n",
      "| epoch  13 |     0/ 2408 batches | accuracy    0.223 | loss 5.4094954 | time  0.08s \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     previous_model \u001b[38;5;241m=\u001b[39m sizes[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(HYPERPARAMETERS)\n\u001b[0;32m---> 19\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43mHYPERPARAMETERS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata_handler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprevious_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 221\u001b[0m, in \u001b[0;36mTrainer.start\u001b[0;34m(self, hyperparameters, data_handler, previous_model)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, hyperparameters[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEPOCHS\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m    219\u001b[0m     epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m--> 221\u001b[0m     train_accu_pct, train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    222\u001b[0m     train_accu_pcts\u001b[38;5;241m.\u001b[39mappend(train_accu_pct)\n\u001b[1;32m    223\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[0;32mIn[1], line 133\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, optimizer, criterion, dataloader, epoch)\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;66;03m# stats for the entire dataset\u001b[39;00m\n\u001b[1;32m    131\u001b[0m running_loss, running_acc, running_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 133\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (answer, clue) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(dataloader):\n\u001b[1;32m    134\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    135\u001b[0m     predicted_answer \u001b[38;5;241m=\u001b[39m model(clue)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ml_dev/lib/python3.9/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ml_dev/lib/python3.9/site-packages/torch/utils/data/dataloader.py:678\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    677\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 678\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    679\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    680\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/envs/ml_dev/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     53\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 91\u001b[0m, in \u001b[0;36mDataHandler.collate_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m     88\u001b[0m     answer_list\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39manswer_pipeline(answer))\n\u001b[1;32m     90\u001b[0m answer_list \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(answer_list)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 91\u001b[0m clue_list \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclue_list\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m answer_list, clue_list\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer = Trainer()\n",
    "\n",
    "embed_hidden_sizes = [(8,50)] \n",
    "\n",
    "for i in range(len(embed_hidden_sizes)):\n",
    "    sizes = embed_hidden_sizes[i]\n",
    "    HYPERPARAMETERS = {\n",
    "        'EPOCHS': 25,\n",
    "        'LR': 0.001,\n",
    "        'BATCH_SIZE': 256,\n",
    "        'EMBEDDING_LAYER_SIZE': sizes[0],\n",
    "        'HIDDEN_SIZE': sizes[1]\n",
    "    }\n",
    "    previous_model = None\n",
    "    if len(sizes) > 2:\n",
    "        previous_model = sizes[2]\n",
    "    \n",
    "    print(HYPERPARAMETERS)\n",
    "    trainer.start(HYPERPARAMETERS, data_handler, previous_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e42449-f504-4849-8b41-0fd98137dff3",
   "metadata": {},
   "source": [
    "## Sample from the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78c87adc-1a5f-4f19-a24c-811babf70d54",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capital of canada: hanoi\n"
     ]
    }
   ],
   "source": [
    "# sample from the model\n",
    "def predict(clue):\n",
    "    with torch.no_grad():\n",
    "        # create clue tensor and pad\n",
    "        clue_indicies = data_handler.clue_pipeline(clue)\n",
    "        clue_indicies += [PADDING_TOKEN_INDEX] * (PAD_TO_SIZE - len(clue_indicies))\n",
    "        output = trained_model(torch.tensor(clue_indicies))\n",
    "        return output.argmax(1).item()\n",
    "    \n",
    "def load_model(training_dir):\n",
    "    with open(os.path.join(training_dir, 'training-results.json'), 'r') as f:\n",
    "        result = json.load(f)\n",
    "        last_epoch = len(result['dev_losses'])\n",
    "        embed_size = result['hyperparameters']['EMBEDDING_LAYER_SIZE']\n",
    "        hidden_size = result['hyperparameters']['HIDDEN_SIZE']\n",
    "\n",
    "        trained_model = CrosswordModel(len(clues_vocab), embed_size, PAD_TO_SIZE, hidden_size, len(answers_vocab), device)\n",
    "        trained_model.load_state_dict(torch.load(os.path.join(training_dir, f'model-epoch-{last_epoch}.pt')))\n",
    "        \n",
    "        return trained_model\n",
    "\n",
    "# load model from saved file\n",
    "training_dir = 'training_results/20230321-100932-training'\n",
    "trained_model = load_model(training_dir)\n",
    "trained_model = trained_model.to(\"cpu\")\n",
    "trained_model.eval()\n",
    "\n",
    "test_clue = 'capital of canada'\n",
    "print(f'{test_clue}: {data_handler.answers_vocab.get_itos()[predict(test_clue)]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
